# 操作系统

管理计算机硬件与软件资源的程序，屏蔽了硬件层的复杂性，由内核负责内存、硬件设备、文件系统、应用程序的管理，成为连接应用程序与硬件的桥梁

## 功能

- 线程与进程：进程的创建、撤销、阻塞、唤醒，进程间通信
- 存储：内存管理，外部存储(磁盘)管理
- 文件：文件的读、写、删除
- 设备：I/O 设备、外部存储设备的管理
- 网络：管理计算机网络的使用，管理计算机网络的配置、连接、通信、安全等，提供高效可靠的网络服务
- 安全：用户的身份认证、访问控制、文件加密等，防止非法用户对系统资源的访问和操作

## 用户态与内核态

- 用户态：处于用户态的线程**能够访问应用程序的数据**，但要读写磁盘、网络通信时**需要发起系统调用**，等待操作系统由用户态转为内核态完成相关请求后返回结果并切换回用户态
- 内核态：处于内核态的线程能够访问操作系统的所有系统资源，**不受任何限制**，内核态拥有更高的权限，能够执行更加底层、敏感的操作，但也需要**更高的开销**

### 区分用户态与内核态的好处

- 安全性：避免了应用程序直接操作系统资源而造成的对操作系统的破坏(内存分配，I/O 处理，时钟设置)
- 性能：只有一个内核态时，所有的进程都共享系统资源(内存、CPU)导致竞争激烈，降低效率，且所有进程的权限相同使得进程能互相直接访问，降低安全性

### 用户态与内核态的切换

- 系统调用：应用程序需要执行特权指令(磁盘读写、网络通信)，故主动向操作系统发起系统调用，操作系统由此切换至内核态处理。系统调用的核心机制是利用操作系统特别开放的中断操作
- 中断：外部设备(I/O 设备)向 CPU 发出的中断信号，CPU 收到后会停止下一条指令的执行，转而执行与中断请求相关的应用程序的指令，若之前执行的是应用程序的指令，那会自然地从用户态切换至内核态进行处理。如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作
- 异常：操作系统运行时发生用户态无法处理的异常，需要切换至内核态进行处理(缺页异常)

#### 系统调用

应用程序需要完成与系统资源相关的操作时(内存管理、设备管理、进程管理等)，会调用操作系统提供的指令，请求操作系统代为执行

- 设备管理：外部设备的请求与释放
- 内存管理：内存分配、回收
- 进程管理：进程的创建、撤销、阻塞、唤醒、通信
- 文件管理：文件的读写、创建、删除

即通过系统调用，应用程序获得了有限的操作系统资源的能力

流程：

1. 用户态的应用程序发起系统调用，由于系统调用中存在特权指令，权限不够的应用程序发起中断(Trap)
2. CPU 执行程序被中断，跳转至中断处理程序，切换至内核态执行系统调用指令
3. 内核态完成请求后主动触发 Trap，切换回用户态继续执行应用程序工作

![系统调用的过程](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/system-call-procedure.png)

## 进程与线程

- 进程：进程即一个程序实例，包含了程序执行所需的资源，多个进程之间的资源不共享
- 线程：线程是系统调度的最小单位，一个进程可以包含多个线程，多个线程之间共享同一个进程的资源(堆、方法区)，线程自身也有私有的资源(程序计数器、虚拟机栈、本地方法栈)，线程之间的通信与调度开销远小于进程

### 进程 vs 线程

- 资源共享：进程的资源是相互独立的；线程之间共享同一个进程的资源，线程也有自身私有的资源
- 开销：进程的创建、回收、调度的开销远大于线程，一个进程能创建多个线程
- 安全性：进程的安全性优于线程(资源独立)

### 线程的必要性

- 性能：
  - 线程更加轻量，一个进程内部能创建多个线程
  - 线程之间共享进程资源，通信不需要经过内核态
  - 切换线程的开销远小于切换进程
- 并发：一个线程只能做一件事，多个线程可以利用多核 CPU 并发处理多件事

### 线程同步方式

- 互斥锁：只有一个线程能够持有锁，阻塞其他所有线程，保证同一时间只有一个线程能够操作资源
  - synchronized
  - Lock
- 读写锁：由读、写操作区分读锁与写锁，读锁可以由多个线程持有，写锁只能由一个线程持有
  - ReentrantReadWriteLock
- 信号量：由信号量来确定同时能有多少线程操作资源
  - 生产者-消费者模型
- 屏障：类似同步点，线程执行至屏障处会阻塞，直到所有线程都到达屏障后才能继续执行
  - CountDownLatch
  - CyclicBarrier
- 事件：使用 notify/wait 来控制线程访问，便于实现多线程优先级的比较操作
  - Condition

### PCB

Process Control Block，进程控制块，操作系统创建进程时会分配 pid 与对应的 PCB，用于获取/管理进程信息

- 进程描述信息：进程名、标识符
- 进程运行信息：进程阻塞原因、进程状态(READY/RUNNING/BLOCK/WAIT/CANCEL)、进程优先级
- 进程占用的资源：进程打开的文件、需求的 CPU 时间、内存空间、I/O 资源

### 进程状态

- NEW：进程刚被创建出来
- READY：进程准备好运行所需的资源，等待 CPU 分配
- RUNNING：进程被 CPU 执行
- WAITING：进程等待运行所需的资源(锁/CPU)而阻塞
- TERMINATED：进程结束/中断退出运行

![进程状态图转换图](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/state-transition-of-process.png)

### 进程通信

进程之间是独立的，要实现进程间通信必须依赖系统的内核作为中间人处理，即在内核中开辟缓冲区，两个进程通过对其读写实现通信

![](https://upload-images.jianshu.io/upload_images/1281379-76c95f147203c797.png?imageMogr2/auto-orient/strip|imageView2/2/w/222/format/webp)

#### 匿名管道

在具有亲缘关系的进程(父子、兄弟)之间，通过内核缓冲区的读写进行通信

![](https://upload-images.jianshu.io/upload_images/1281379-05378521a7b41af4.png?imageMogr2/auto-orient/strip|imageView2/2/w/228/format/webp)

- 单向通信：数据只能向一个方向流动，双方读写必须建立两个管道
- 亲缘关系：只能在父子、兄弟进程间使用
- 独立的文件系统：管道本质是缓冲区，对进程而言就是在文件中进行读写，管道不是普通的文件，不属于某种文件系统，而是单独构成一种文件系统，只存在于内存中
- FIFO：写入的数据在管道尾部，读取的数据在管道头部，不能随意更改读写的位置

管道本质上是一个 FIFO 循环队列的缓冲区，使用类似生产者-消费者模型来处理缓冲区已满/已空的情况(已满时阻塞生产者，已空时阻塞消费者，在处理后唤醒)

阻塞问题：无名管道无需显式打开，创建时直接返回文件描述符，在读写时需要确定对方的存在，否则将退出。

- 当前进程向匿名管道的一端写数据，必须确定另一端有某一进程
- 若写入匿名管道的数据超过其最大值，写操作将阻塞
- 若管道中没有数据，读操作将阻塞
- 若管道发现另一端断开，将自动退出

缺点：

- 单向
- 亲缘
- 有限：管道存在于内存中，创建时为缓冲区分配一个页面大小
- 匿名
- 格式：管道传输的是无格式字节流，需要双方事先约定好数据格式(一个消息/命令/记录包含多少字节)

#### 有名管道

通过以文件形式将缓冲区放在文件系统中，得到一条访问路径，即使没有亲缘关系的进程也能通过路径来对缓冲区进行读写

有名管道也是严格遵守 FIFO

阻塞问题：有名管道在打开时需要确定对方的存在，否则将阻塞

- 以读方式打开管道，在此之前必须一个进程以写方式打开管道，否则阻塞
- 以读写方式打开有名管道，即以当前线程读，当前线程写，不会阻塞

#### 信号(Signal)

信号是 Linux 系统中用于进程间通信的一种机制，信号的发送方不需要关注接受方的状态(任何时候都可以发送)

若接收方未处于执行状态(READY/WAITING)或阻塞信号的接收，则由内核保存信号，直到接收方开始执行时再传递；若接收方已退出(TERMINATED)，则内核直接丢弃信号

信号是**软件层次上对中断机制的模拟**，是一种**异步通信**方式，信号可以在用户空间进程和内核之间直接交互，内核可以利用信号来通知用户空间的进程发送了哪些系统事件，信号的产生可以是硬件的输入(ctrl + c，硬件异常，无效的存储访问)，或软件指令(kill)

信号的生命周期：

1. 发送方生成信号，设置接收方信息(pid)，传递给操作系统
2. 操作系统根据接收方状态选择性地发送
   - 阻塞：内核保存信号，等待接收方解除阻塞
   - 执行：内核传递信号
   - 退出：接收方进程已退出，则内核直接丢弃信号
3. 接收方收到内核传递的信号，中断当前执行的程序，保存上下文(寄存器数据、程序执行位置、CPU 状态)，根据信号中设置的预处理方式执行中断服务程序，完成后恢复到中断的位置

#### 消息队列

消息队列是存放在内核中的消息链表，每个消息队列都有一个消息队列标识符

- 存储：消息队列存放在内核中，只有内核重启、手动删除才会被删除
- 访问：消息队列遵循 FIFO 原则，但也能实现随机访问
- 使用：消息队列不需要读写双方的注册，即不同于管道，消息队列的写操作不需要另一个进程在队列等待消息到达
- 并发：消息队列支持多个进程读写
- 对比：
  - 管道：传输的信息具有特定的格式(管道为无格式字节流)，不受缓冲区大小限制，读写双方独立进行
  - 信号：能够传输更多信息量

#### 共享内存

进程间直接读写同一块内存空间，即内核中专门开辟一块内存区，进程将该内存地址映射到自己的私有地址空间，之后通过直接修改内存数据进行通信，而不需要进行数据拷贝，是**效率最高的通信方式**，需要额外的同步方式确保同步安全(信号量)

![](https://upload-images.jianshu.io/upload_images/1281379-adfde0d80334c1f8.png?imageMogr2/auto-orient/strip|imageView2/2/w/538/format/webp)

#### 信号量

信号量是一个计数器，用于多线程访问共享数据时的进程间同步

为了获取共享资源，进程需要：

1. 创建信号量：调用者指定初始值
2. 等待信号量：测试信号量的值，若小于 0 则阻塞(P 操作)
3. 挂出信号量：将信号量的值加 1(V 操作)

P/V 操作都是原子操作，故信号量通常在内核实现

![两个进程使用一个二值信号量](https://upload-images.jianshu.io/upload_images/1281379-376528c40d03717e.png?imageMogr2/auto-orient/strip|imageView2/2/w/635/format/webp)

![两个进程使用一个 Posix 有名二值信号量](https://upload-images.jianshu.io/upload_images/1281379-a72c8fbe22340031.png?imageMogr2/auto-orient/strip|imageView2/2/w/613/format/webp)

![一个进程两个线程共享基于内存的信号量](https://upload-images.jianshu.io/upload_images/1281379-a1b276fae9db985d.png?imageMogr2/auto-orient/strip|imageView2/2/w/284/format/webp)

##### 信号量 vs 互斥量

- 用途：
  - 互斥量用于线程的互斥，保证同一时间只有一个线程能够操作资源，具有唯一性与排他性，但无法限制访问顺序(无序)
  - 信号量用于线程的同步，在互斥的基础(大多数情况)通过其他机制实现访问者对资源的有序访问

#### 套接字(socket)

socket 是一种通信机制，客户/服务器(进行通信的进程)系统的开发工作既可以在本地单机进行，也可以跨网络进行，即可以让两个计算机进程通过网络连接

![Socket 是应用层和传输层之间的桥梁](https://upload-images.jianshu.io/upload_images/1281379-2db1deb0115ec4f2.png?imageMogr2/auto-orient/strip|imageView2/2/w/319/format/webp)

socket 是 TCP/IP 通信的基本操作单元，不同主机间的进程进行双向通信的端点，通信双方通过 socket 中的相关函数完成通信过程

socket 由 3 个属性确定：域、端口号、协议类型

##### 域

指定 socket 通信中的网络介质：

- AF_INET：Internet 网络，需要服务器的 IP 和端口号指定一台联网机器上的某个特定服务，所以在使用 socket 作为通信的终点，服务器应用程序必须在开始通信之前绑定一个端口，服务器在指定的端口等待客户的连接
- AF_UNIX：UNIX 文件系统，即文件的输入/输出，以文件名为地址

##### 端口号

每个基于 TCP/IP 网络通信的进程都被赋予了唯一的端口和端口号，端口是一个信息缓冲区，用于保留 socket 中的输入/输出信息，端口号用于区别主机上的每个进程

每个 socket 通过组合进程 IP 和端口来区别

##### socket 协议类型

Internet 提供 3 种通信机制：

- 流套接字：在域中通过 TCP/IP 连接实现，也是 AF_UNIX 中常用的类型。流套接字提供一个**有序、可靠、双向字节流**的连接，可以确保发送的数据不会丢失、重复、乱序，且有一定的出错重传机制
- 数据包套接字：不需要建立与维护连接，在域中通过 UDP/IP 协议实现。对可发送的数据长度有限制，数据包作为单独的网络消息被传输，可能丢失、重复、乱序，即 **UDP 不是一个可靠的协议，但是效率更高(不需要建立连接)**
- 原始套接字：**允许对较低层次的协议直接访问**(IP、ICMP)，常用于检验新的协议实现，或访问现有服务中配置的新设备，因为原始套接字能够自如地控制 Windows 下的多种协议，能够对网络底层的传输机制进行控制，所以可以应用原始套接字操作网络层和传输层应用
  - 原始套接字可以读写内核没有处理的 IP 数据包，流套接字只能读取 TCP 协议的数据，数据套接字只能读取 UDP 协议的数据

##### socket 通信的建立

- 服务器：
  1. 服务器应用程序通过系统调用创建一个 socket，它是系统分配给服务器进程的类似文件描述符的资源，不能与其他进程共享
  2. 进程给 socket 命名(系统调用 bind)后开始等待客户连接到该 socket
  3. 系统调用 listen 来创建队列，用于存放来自客户的连接
  4. 服务器通过系统调用 accept 接受客户连接，创建一个与原有的命名 socket 不同的新 socket，该 socket 只用于与这个特定客户通信，而命名 socket(原 socket) 保留下来继续处理来自其他客户的连接
- 客户端：
  1. 客户应用程序首先调用 socket 创建未命名 socket，将服务器的命名 socket 作为地址来调用 connect 与服务器建立连接
  2. 建立连接后可以像使用底层的文件描述符一样用 socket 实现双向数据的通信(通过流进行数据传输)

![socket 通信基本流程](https://upload-images.jianshu.io/upload_images/1281379-2575b81bbab6b67b.png?imageMogr2/auto-orient/strip|imageView2/2/w/437/format/webp)

### 进程的调度算法

- FCFS：先到的请求先服务，直到执行完毕或被阻塞中断
- SJF：短时间任务优先
- RR：绝对公平地为每个任务分配时间片轮询
- MFQ：多级反馈队列调度算法，既能使高优先级的作业得到响应又能使短作业迅速完成
- Priority：为每个任务分配优先级，首先执行具有最高优先级的进程

### 僵尸进程与孤儿进程

父进程通过 fork() 创建子进程，子进程是父进程的副本，有自己的资源与 PCB，父子进程的运行相互独立，且当父进程通过系统调用 exit() 结束生命时，内核会释放父进程的所有资源(内存，文件)，但进程的 PCB 仍然存在，只有父进程调用 wait()/waitpid() 时才会被回收，以便父进程获取子进程状态

- 僵尸进程：子进程已经结束，但父进程没有及时调用 wait() 导致子进程的 PCB 一直存在于系统中，无法被进一步使用
- 孤儿进程：父进程已经结束(意外终止，未及时调用 wait())，子进程仍在运行。为了避免孤儿进程占用系统资源，操作系统会将孤儿进程的父进程设置为 init 进程(进程号为 1)，由 init 进程来回收孤儿进程的资源

## 死锁

死锁即多个线程都需要对方持有的资源才能继续执行，但每个线程都不会主动放弃持有的资源，由此所有线程的执行都被无限期地阻塞

### 产生死锁的必要条件

- 占用并等待：线程不会主动放弃资源
- 非抢占：线程持有的资源不会被夺取
- 循环等待：线程的资源请求链形成环
- 互斥：线程请求的资源不可共享

死锁的产生必然满足上述的所有条件

### 解决死锁的方法

- 预防：采用某种策略，限制并发进程对资源的请求，从而使得死锁的必要条件在系统执行的任何时间上都不满足
- 避免：系统在分配资源时，根据资源的使用情况提前做出预测，从而避免死锁发生
- 检测：系统设有专门的机构，检测并定位死锁有关的进程与资源
- 解除：将进程从死锁状态下解脱

#### 预防

破坏死锁的产生条件：

- 互斥：使得资源可以同时访问
- 非抢占：采用剥夺式调度算法，一般仅适用于主存资源和处理器资源的分配，会导致资源利用率下降

能够预防系统发生死锁，但会导致低效的进程运行和资源利用率

##### 静态分配策略

一个进程必须在执行前获取到执行所需的全部资源，即要么占有所有资源，要么全部放弃(破坏占有并等待的条件)

静态分配策略逻辑简单，实现容易，但这种策略严重地降低了资源利用率，因为在每个进程所占有的资源中，有些资源是在比较靠后的执行时间里采用的，甚至有些资源是在额外的情况下才使用的，这样就可能造成一个进程占有了一些**几乎不用的资源而使其他需要该资源的进程产生等待**的情况

##### 层次分配策略

所有资源被分成多个层次，一个进程得到某一层的一个资源后，它只能再申请较高一层的资源；当一个进程要释放某层的一个资源时，必须先释放所占用的较高层的资源(破坏循环等待，因为已经获取到高层资源的进程无法申请低层资源)

#### 避免

运行存在死锁产生的条件，通过合理的资源分配避免死锁

- 安全状态：存在一种资源分配方案，令每个进程在有限时间内都能获取到满足执行的资源

每当在为申请者分配资源前先测试系统状态，若系统资源分配给申请者可能产生死锁，则拒绝分配

##### 银行家算法

当一个进程申请使用资源时，通过先试探分配给该进程资源，然后通过安全性算法判断分配后系统是否处于安全状态，若不安全则试探分配作废，让进程继续等待；若能进入安全状态，则真正分配资源

![](https://img-blog.csdn.net/20180508204335770?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzNDE0Mjcx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

1. P1 申请资源，银行家算法先试探地分配，若空闲资源不够，直接拒绝
2. 判断分配给 P1 后剩余的资源是否能够满足进程队列中的某个进程执行完毕，若没有则为不安全状态，可能出现死锁
   - 已被满足的进程会将持有的资源返回，但 P1 一次申请的资源未必是满足完全执行的

#### 检测

对资源的分配加以限制可以预防/避免死锁发生，但不利于各进程对系统资源的充分共享，解决死锁问题的另一条途径是死锁检测和解除

- 预防/避免 vs 悲观锁：预防/避免方案类似悲观锁，总是假定死锁一定会发生，故分配资源十分谨慎
- 检测/解除 vs 乐观锁：检测/解除方案类似乐观锁，只会在死锁发生后才进行处理

检测对资源的分配不加任何限制，也不采取死锁避免措施，但系统定时运行一个死锁检测程序，判断系统内是否存在死锁，再采取措施去解除

##### 进程-资源分配图

操作系统中的每一时刻的系统状态都可以用进程-资源分配图来表示，进程-资源分配图是描述进程和资源申请及分配关系的一种有向图，可用于检测系统是否处于死锁状态

![进程-资源分配图](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/process-resource-allocation-diagram.jpg)

由进程-资源分配图可以看出资源的分配是否存在占有和等待资源的环路

##### 检测步骤

1. 判断进程-资源分配图是否存在环路
2. 进程-资源分配图中有环路，且每个资源类仅有一个资源，则系统中已发生死锁
3. 进程-资源分配图中有环路，且涉及到的资源类有多个资源
   1. 若能在进程-资源分配图中找到一个既不阻塞又非独立的进程，该进程能够在有限的时间内归还占有的资源，即把边消除，重复此过程，直到有限时间内消除所有边，则不会发生死锁

#### 解除

- 结束所有进程执行，重启操作系统：丢失所有工作，损失大
- 撤销涉及死锁的所有进程，解除死锁后继续执行：丢失进程工作结果
- 逐个撤销涉及死锁的进程，回收资源直至死锁消除
- 抢占资源：从涉及死锁的进程中抢占资源再分配直至死锁解除

## 内存管理

- 内存的分配与回收：为进程分配与回收内存
- 地址转换：将内存中的物理地址映射为程序的虚拟地址
- 内存扩充：系统没有足够内存时，利用虚拟内存技术或自动覆盖技术，**从逻辑上扩充内存**
- 内存映射：**将一个文件直接映射到进程的进程空间**，通过内存指针用读写内存的办法直接存取文件内容，速度更快
- 内存优化：通过调整内存分配策略和回收算法来优化内存使用效率
- 内存安全：保证进程之间使用内存互不干扰，避免恶意程序通过修改内存来破坏系统的安全性

### 内存碎片

内存碎片由内存的申请与释放产生，会导致内存的使用效率降低

- 内部内存碎片：采用固定比例(2 的幂次方)分配内存时，进程可能获得比需要的更多的内存，这部分进程内未使用的内存为内部内存碎片
- 外部内存碎片：由于内存的回收未整理，导致产生大量零散的小空间内存，这些内存无法满足进程的需要，属于进程外部的无法使用的内存

![内存碎片](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/internal-and-external-fragmentation.png)

### 内存管理

#### 连续内存分配

为进程分配一个连续的内存空间，将内存分为几个固定大小的块，每个块中只包含一个进程。

如果进程只需要很小的空间，但仍会被分配到一块相对较大的内存，导致内部内存碎片过大

分配的两个内存块之间也会存在外部内存碎片，这些不连续的内存过小，无法再继续分配

##### 伙伴系统

Linux 中使用伙伴系统解决连续内存分配的内部内存碎片问题

伙伴系统将相邻的内存块视为伙伴，为进程分配内存时会先寻找大小合适的内存块，若选择的块过大，会将其等分为 2 份，一直递归到合适的大小

当相邻的内存被释放时，会将两者合并成较大的内存块(减少外部内存碎片)，由此减少内存碎片

伙伴系统只能分配大小为 2^n  的内存块，故仍存在内部内存碎片的问题

![伙伴系统内存浪费问题](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/buddy-system-memory-waste.png)

#### 非连续内存管理

非连续内存管理允许进程使用离散的内存

- 段式管理：将进程分为若干个具有实际意义的段(连续的物理内存)，每个段的内存位置是离散的，即主程序段 MAIN，子程序段 X 分配在不连续的内存地址。由逻辑地址得到段号、段内地址，再根据段号、段内地址求出基址，基址 + 段内地址 = 物理地址

  - 优点：可以分别编写和编译，可以针对不同类型的段采取不同的措施，可以按段为单位进行共享
  - 缺点：产生外部碎片，进程必须全部装入内存

  ![](https://img-blog.csdnimg.cn/20200402124854451.png)

- 页式管理：将物理内存分为连续等长的物理页，进程的虚拟地址空间也被划为连续等长的虚拟页，位移量的位数决定页的大小，通过页表映射找到页对应的内存块

  - 优点：没有外碎片，每个内碎片不超过页大小
  - 缺点：程序全部装入内存，要求有相应的硬件支持(缺页中断)

  ![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWctbXkuY3Nkbi5uZXQvdXBsb2Fkcy8yMDEyMTAvMTUvMTM1MDI5ODI0MF80MTI3LmpwZw?x-oss-process=image/format,png)

- 段页式管理机制：结合段式与页式管理的一种内存管理机制，把物理内存先分成若干段，每个段又分为若干大小相等的页。需要段表管理内存分配与释放、缺段处理，每个段需要页表把段中的虚页转换为内存中的实际页面，页表也需要缺页中断和页面保护

  - 优点：兼具段式与页式的优点
  - 缺点：实现复杂

  ![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9zdGF0aWMwMS5pbWdrci5jb20vdGVtcC8yMmQyM2RlNTc2MzE0ZDg4ODBkYTNlNTc1NjhlYjRlYS5wbmc?x-oss-process=image/format,png)

### 虚拟内存

虚拟内存是一种逻辑上的内存，用于作为简化进程与物理内存交互的桥梁，CPU 通过 MMU(Memory Management Unit)将虚拟地址转换为物理地址后，再通过总线传到物理内存设备，进而完成相应的物理内存读写请求

- 隔离进程：进程使用物理内存映射到虚拟内存的地址，每个进程都认为拥有整个物理内存，进程之间彼此隔离，
- 物理内存利用率：只需要加载进程使用到的数据的内存
- 简化内存管理：使用虚拟内存地址而不需要关注实际的物理内存地址
- 共享物理内存：多个进程共享的物理内存地址只需要加载一次
- 提高内存使用安全性：控制进程对物理内存的访问，用户应用程序无法访问操作系统使用的内存空间
- 更大的可用内存空间：系统内存不足时，可以将磁盘的地址映射为虚拟内存空间

### 虚拟地址 -> 物理地址

MMU 进行转换的机制主要有：

- 分段
- 分页
- 段页

#### 分段机制

分段机制以段的形式管理/分配物理内存，应用程序的虚拟地址空间被分为大小不等的段，段是有实际意义的，每个段定义了一组逻辑信息(主程序 MAIN，子程序 X)

![](https://img-blog.csdnimg.cn/20200402124854451.png)

虚拟地址由段号与段内地址组成：

- 段号：标识着该虚拟地址属于整个虚拟地址空间中的哪个段
- 段内地址：相对于该段起始位置的偏移量

具体的地址翻译过程：

1. MMU 解析得到虚拟地址的段号
2. 由段号找到对应的段表
3. 将段表的基址 + 段内地址得到实际的物理地址

![分段机制下的地址翻译过程](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/segment-virtual-address-composition.png)

通过段号不一定能找到段表：

- 段表被删除：软件错误，恶意软件
- 段表未创建：系统内存不足或无法分配到连续的物理内存块

非连续段的回收会导致段与段之间留下外部碎片：

![分段机制导致外部内存碎片](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/segment-external-memory-fragmentation.png)

#### 分页机制

分页把物理内存分为连续等长的物理页，应用程序的虚拟地址空间也划分为连续等长的虚拟页

分页机制下，应用程序虚拟地址空间中的任意虚拟页可以被映射到物理内存中的任意物理页上，因此可以实现物理内存资源的离散分配。分页机制按照固定大小分配物理内存，使得物理内存资源易于管理，可有效避免分段机制中外部内存碎片的问题

##### 页表

分页管理通过页表映射虚拟地址和物理地址，每个应用程序都有对应的页表

![单级页表](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/page-table.png)

分页机制的虚拟地址通过页号与页内地址组成：

- 页号：由虚拟页号从页表中获取虚拟页表
- 页内地址：页起始地址 + 页内地址 = 物理地址

MMU 翻译具体地址：

1. MMU 将虚拟地址获取虚拟页号
2. 由虚拟页号在应用程序的页表中获取对应的物理页号(页表项)
3. 将页表项 + 页内地址得到物理地址

![分页机制下的地址翻译过程](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/paging-virtual-address-composition.png)

#### 多级页表

单极页表的空间开销大，每个页表项为一个地址，占用 4 字节，且大多数应用程序只用到页表中的几项

多级页表对应多个页表，每个页表与前一个页表关联，二级页表中的一级页表项是一对多的关系，二级页表按需加载(只会用到很少一部分二级页表)，进而节省空间占用

![多级页表](https://oss.javaguide.cn/github/javaguide/cs-basics/operating-system/multilevel-page-table.png)

#### TLB

Translation Lookaside Buffer，转址旁路缓存，存在于 MMU 中的寄存器，存放虚拟页号与物理页号的映射关系

引入 TLB 后的解析流程：

1. MMU 先从 TLB 中找对应的虚拟页号，若找到(hit)则直接返回对应的物理地址
2. 若找不到(miss)，则从内存中的主表查询对应物理页号，再将对应关系写入 TLB
3. 若 TLB 已满，则需要某种策略选择被淘汰的页号

#### 换页机制

当物理内存不够存储时，会将一些物理页转移至磁盘存储，在需要用到时再读取到内存

#### 页缺失

当软件试图访问已映射在虚拟地址空间中，但目前并未被加载在物理内存中的一个分页时，由 MMU 发起的中断

- 硬性页缺失：物理内存中没有对应的物理页，中断处理会指示 CPU 从已打开的磁盘文件中读取相应的内容到物理内存，而后交由 MMU 建立相应的虚拟页和物理页的映射关系
- 软性页缺失：物理内存中已存在对应的物理页，但还未将其与虚拟页建立映射，中断处理会指示 MMU 建立相应的虚拟页和物理页的映射关系
- 无效页缺失：访问的物理内存是无效的

#### 页面置换算法

当发生缺页中断，需要在物理内存新增物理页，但物理内存空间不足时，需要使用页面置换算法决定应将哪些物理页转移至磁盘

- 最优：理论上的算法，每次都替换未来使用最少的物理页(无法预知)
- FIFO：先进先出，性能较差
  - 先进的物理页通常是经常使用/长期存在的
  - 被替换的物理页也会被使用，可能出现分配的物理页数增加，缺页率反而变高(Belady 现象)
- LRU：Least Recently Used，最久未使用
- LFU：Least Frequently Used，最多使用
- TIME：按时间

### 分页 vs 分段

共同点：

- 非连续内存管理方式
- 采用地址映射的方式，将虚拟内存地址映射到物理地址，以实现对内存的管理和保护

不同点：

- 单位：分页以页为单位，是实际存在的大小固定(2 的幂次方)的物理单位；分段以段为单位，是逻辑上的大小不定(由应用程序决定)的逻辑单位
- 碎片：分页因为页大小是固定的，容易出现内部内存碎片；分段由于段与段之间会留下空间，容易出现外部内存碎片
- 映射：分页通过页表完成虚拟地址到物理地址的映射，页表通过一级页表和二级页表实现多级映射；分段采用段表完成虚拟地址到物理地址的映射，每个段表项记录了该段的起始地址和长度信息
- 程序：分页行为是基于硬件的，即对程序没有任何要求，只需要按照虚拟地址进行访问；分段需要手动将程序分为多个段，并且显示地用段寄存器访问不同段

### 段页机制

结合段式管理与页式管理的内存管理机制，把物理内存先分成若干段，每个段又继续分成若干大小相等的页

地址解析流程：

1. 段式地址映射
2. 页式地址映射

### 局部性原理

程序在运行时，对数据与指令的访问存在时间局部性与空间局部性：

- 时间局部性：在一段时间内会重复访问同一块数据与指令
- 空间局部性：程序对数据/指令的访问具有一定的空间连续性，在一段时间内会重复访问与一个数据/指令相邻的数据与指令

在分页中：

- 时间局部性：使用 TLB 缓存一段时间内使用的物理页，提高频繁访问时的效率
- 空间局部性：在缓存中有预测性地存入使用到的数据项/指令相邻的数据/指令

## 文件系统

文件系统负责管理与组织存储设备上的文件与目录

- 存储管理：将文件数据存储到物理设备，管理空间分配，确保存在足够的空间存放且不会产生冲突
- 文件管理：文件的创建/删除/修改/移动
- 目录管理：目录的创建/删除/修改
- 安全管理：管理文件的访问控制权限

### 文件链接

Linux/Unix 中，文件链接是一种特殊的文件类型，可以在文件系统中指向另一个文件。每个文件和目录都有一个唯一的索引节点号(inode)，用于标识文件/目录

#### 硬链接

- 硬链接通过 inode 节点号建立连接，硬链接与源文件的 inode 节点号一致，对文件系统来说是完全平等的，删除其中一个对另一个没有影响(设置硬链接防止误删)
- 文件只有在删除源文件与所有硬链接才会被真正删除
- 不能对目录及不存在的文件创建硬链接，且硬链接无法跨越文件系统
  - 每个文件系统都有自己的 inode 表，跨越文件系统可能产生冲突
- ln 命令创建硬链接

#### 软链接

- 软链接类似快捷方式，其 inode 与源文件不同，而是指向一个文件路径，当源文件被删除时，软链接仍存在，但指向一个无效文件路径
- 软链接可以指向目录与不存在的文件，也可以跨越文件系统
- ln -s 命令创建软链接

### 文件系统性能优化

- 提升硬件：使用高速硬件设备(SSD、NVMe)，使用 RAID 技术提高磁盘性能
- 文件系统选型：对不同的应用场景选择合适的文件系统
- 缓存：使用缓存避免频繁访问磁盘
- 预留磁盘空间：避免磁盘用满影响文件系统
- 合理分区：合理的磁盘分区方案，能够使文件系统在不同区域存储文件，减少文件碎片，提高读写性能

### 磁盘调度算法

操作系统中对磁盘访问请求进行排序和调度，提高磁盘的访问效率

一次磁盘读写操作时间：

- 寻道时间：磁头移动到数据所在磁道所需时间
- 延迟时间：磁盘旋转到数据所在扇区的时间
- 传输时间：传输请求数据的时间(相对较少)

调度算法通过改变磁盘请求的处理顺序，减少磁盘寻道时间和延迟时间

- FCFS：先到的请求先服务，没有算法开销，但效率低
- SSTF：Shortest Seek Time First，最短寻道时间优先，每次先处理距离磁头最近的数据请求(饥饿问题)
- SCAN：扫描/电梯算法，磁头沿着一个方向扫描磁盘，存在请求则处理，直到到达磁盘边界后改变移动方向
  - 刚转向时收到的请求要等下次转向才完成
- C-SCAN：Circular Scan，循环扫描，只在磁盘一侧且直朝一个方向扫描，每次到达边界后回到起点
- LOOK：边扫描边观察，SCAN 算法中即使当前方向没有请求也会继续前进扫描，造成大量浪费，LOOK 算法中若移动方向没有请求，就可以立即改变方向
- C-LOOK：同 LOOK-SCAN 的优化方式，磁头只需要返回到有请求的地方