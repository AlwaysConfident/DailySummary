## 基础认知

### 什么是分布式系统？

分布式系统是一个将组件分布到多个联通的计算机上，各个组件之间通过通信和协调共同完成一个任务的系统，虽然各个组件分布在不同的独立的计算机上，但对于用户来说是一个整体的系统

### 分布式系统的特点？

- 多线程：系统中有多个线程并发进行
- 各组件独立：独立的计算机和操作系统，通过网络通信传递信息来协作
- 系统时钟不共享：不能通过时间确定事件顺序

### 为什么需要分布式系统？

1. 高性能：通过水平拓展来避免硬件升级的瓶颈
2. 高可用：多个组件之间可以及时替换故障节点
3. 可扩展：节点便于增删

### 分布式计算谬误

1. 网络延迟问题
   1. 消息丢失
   2. 超时导致重传和误判不在线
   3. 乱序
2. 带宽
3. 网络安全
4. 拓扑结构改变
5. 单一管理员
6. 传输成本
7. 网络是同构的
8. 部分失效：只有一部分节点正常工作，无法预测任务是否成功，尤其是需要原子性的任务(要求所有节点同时成功/失败)，在网络环境的影响下，消息会存在延迟/丢失的情况，节点之间难以同步状态
9. 时钟问题
   1. 每台机器的时钟不同
   2. 网络传输存在时延

## 两将军问题

两个将军需要同时进攻一处堡垒，但由于派出的信使可能被俘，所以都无法确信对方是否收到消息，导致即使重复发出确认的消息，仍然无法保证消息可靠

该问题表明节点之间只能通过消息传递来确认彼此的状态

工程解：TCP 的三次握手

## 拜占庭将军问题

两将军问题的延申，多个将军协商进攻/撤退，但可能存在叛徒故意散布假消息(发送矛盾的消息)，该问题的主要目的是令忠诚的将领在同一个计划达成共识

## 网络链路模型

网络设备出现故障，但节点仍正常工作，导致节点之间无法通信，形成网络分区

链路：

- 可靠链路：不会丢失，不会凭空生成消息，可能乱序
- 公平损失链路：可能丢失，可以通过重发升级为可靠链路
- 任意链路：可能丢失，也可能被攻击者篡改、伪造数据，通过 TLS 等手段升级为公平损失链路

## 节点故障模型

节点可能在发送/接收消息时出现故障，导致消息丢失

节点故障类型：

- 崩溃-停止：节点发生硬件上的故障导致崩溃，一般来说进程失败后就永久退出运行状态，不再加入系统
- 崩溃-恢复：节点故障后可以通过重启等手段恢复并重新加入系统，通常需要日志、检查点等持久化存储进行恢复
- 拜占庭问题：节点故障后可能会令算法偏移甚至故意破坏算法

## 时分模型

根据消息的响应时间分为同步/异步模型

- 同步：消息在一段可预知的时间内响应，简单但不可靠，因为节点和网络可能出现故障
- 异步：无法预知消息何时响应，基于异步的算法更加健壮，且适用于同步场景，但存在 FLP 不可能问题
- 部分同步

## 消息传递语义

由于网络/节点可能存在故障，导致消息丢失，故需要进行重发，但重发的多个消息可能被执行多次，虽然可以通过幂等操作令其只生效一次，但需要一直等待确认请求来保证每个操作都是幂等的，开销较大

通过消息的唯一标识来避免多次执行：

- At Most Once：消息严格传递一次，即使丢失也不重发
- At Least Once：消息至少发送一次，丢失后会重发
- Exactly Once：消息精确发送一次，不会丢失，也不会重发，不现实，通过忽略重发来达到该效果

## 分布式数据

随着数据量的激增，单机下的数据库无法存储并提供满足要求的性能，故需要将数据库进行分区再部署至多台机器

### 分区

分片即将原数据库表进行分割，根据分割方向不同有：

- 水平分区：保持表结构，将一张表的数据分成多张表
- 垂直分区：按表的列进行分割，由于表的列有限，故分割的子表也有限

垂直/水平分区在查询数据时需要联接多个节点中的数据，考虑到节点间通信会十分低效

### 水平分区算法

- 范围算法：取表中的一列作为索引，如年份，进行分割，将对应范围内的数据都存储到一个节点中
  - 简单易用
  - 可以/仅能对索引列进行范围查找
  - 数据可能分布不均，导致出现热点数据，如某一年的数据激增
- 哈希算法：取表中一列数据进行哈希运算，根据哈希结果存储至不同的节点
  - 数据分布较为均匀
  - 无法进行范围查询
  - 节点数量改变时需要重新计算哈希，并且重新分配数据
- 一致性哈希：将哈希值作为一个环，节点分布在环上，根据计算出的哈希值取顺时针的第一个节点
  - 修改节点数量时，仅影响与新节点相邻的节点
  - 由于节点取顺时针第一个，所以当一个节点失效时，数据会全部转移至下一个节点，容易导致热点数据和过高荷载
    - 虚拟节点：从真实的物理节点中产生若干个逻辑上的复制节点分布至环上，当有节点出现故障时可以避免全部流向一个节点

### 复制

将同一份数据备份至多个节点中，可以提高可用性、吞吐量，但也引入了数据同步的问题

### 复制算法

- 单主复制（主从同步）：只有一台主节点负责写操作，其他从节点负责同步主节点数据与提供读操作，主从节点间的同步分为三种：
  - 同步：主节点完成写操作后会等待所有从节点完成该数据同步，之后才返回写入成功的响应
    - 可以确保写入的数据不会丢失
    - 但在有从节点存在阻塞时，写效率较低
  - 异步：主节点不必等待从节点同步完成
    - 可能丢失数据
    - 写效率较高
  - 部分同步：主节点等待至少若干个从节点同步完成
  - 单主复制简单、易用，由于所有写操作都在主节点中，所以可以确保写操作的顺序性，且便于进行事务操作
  - 单主复制适用于读多写少的场景
  - 由于只有一个主节点，故当该节点故障时，如何重新选举主节点，确保数据的一致性，避免脑裂问题
- 多主复制：具有多个主节点，提高了写效率，同时可以避免主节点的单点故障，但主节点之间需要进行数据同步，同时由于网络延迟，各个主节点之间的执行顺序可能不同，导致出现数据冲突
  - 冲突避免：通过将用户的请求与特定的节点绑定（哈希），避免同一份数据在多个节点修改
  - 冲突处理：没有一种模型能够覆盖所有冲突情况，故大多数情况下多主复制带来的复杂性要高于便利性
    - 客户端处理：将冲突数据返回给客户端，由用户决定正确的数据（购物车）
    - 最后写入获胜（LWW）：将写入操作打上时间戳/ID，发生冲突时取最新的一个操作，但由于各个节点之间的机器时间可能不同，故该方法也存在误差
    - 因果律：判断写入操作之间的因果关系（happens-before）来决定合适的写入操作，但也存在没有因果关系的操作
- 无主复制：节点不再区分主从，写操作会向多个（甚至全部）节点发送，需要指定数量的节点响应成功后才视为写入完成，而读操作也会向多个节点请求
  - 不存在主节点也就不存在选举问题，但节点之间的数据一致性更加复杂
    - 读恢复：客户端处理的一种，由于读操作也会发往多个节点，故获取的多个结果中可能存在差异，此时由客户决定保留哪个并更新节点，但对于长期没有被读到的数据则可能丢失
    - 反熵：节点之间利用哈希树来快速比较数据的差异部分并修复，反熵过程不保证写操作顺序，只保证操作结果的一致
  - Quorum 算法：用于测算读写操作应该得到多少节点的响应才算成功的一致性算法
    - 对于 N 个节点，成功写入 W 个节点，读取 R 个节点，需要保证 W+R>N 且 W>N/2，由此保证每次读取数据都至少有一个节点是成功写入的，并且两个不同的写操作不会同时作用于一份数据

### CAP 理论

CAP 理论表明了无法构建完美的分布式模型，在进行架构设计时需要从三个方面进行取舍：

- 一致性：用户从所有节点读到的数据都是一致的
- 可用性：节点在没有故障时可以正常响应请求
- 分区容错性：在发生网络分区时系统能够正常工作

系统需要在发生网络分区时可以正常运作，故设计时一般选择 CP/AP

但是因为网络分区并非频繁发生，所以不必总是在一致性与可用性之间取舍，只需要在发生网络分区时进行判断，且 CAP 忽略了节点之间的网络延迟，当延迟较高时，放弃处理请求 --> 选择可用性，阻塞等待 --> 选择一致性， 例如在单主复制的场景，可以选择所有从节点都同步完成后再响应(CP)，也可以选择异步地在主从之间进行同步(AP)

### PACELC 理论

发生网络分区(P)时，考虑可用性(A)和一致性(C)之间的取舍，否则(E)考虑延迟(L)和一致性(C)之间的取舍

### 一致性模型

一致性：在并发编程中，系统与开发者之间的一种约定，当满足该约定时，读写操作的结果都是可预测的，可预测保证了系统的逻辑正确性

#### 线性一致性

线性一致性是一种强一致性模型，在保证操作原子性和全局时序的情况下，令客户端将系统看作只有一个节点

将所有操作按时间线顺序排序，其中没有重叠的部分不可重新排序，而有重叠的部分(并发)可以任意重排序，当存在一种排序使得所有操作都合法(读操作能够正确读取到最近一次写操作结果)时可以视为符合线性一致性

#### 顺序一致性

顺序一致性稍弱于线性一致性，它关注局部的有序，只要求每个客户端自身的执行顺序不可重排序，但多个客户端之间可以任意排序，故减少了维护全局时钟带来的开销

#### 因果一致性

因果一致性弱于顺序一致性，它只要求含有因果关系的操作顺序不可变，没有因果关系的操作可以重排序

#### 最终一致性

最终一致性是最弱的一致性模型，它不要求读操作必须返回最新的写操作结果，线程的操作也可以任意排序，只需要保证经过一段时间后，系统内的各个节点数据可以达到一致

#### 面向客户端的一致性

面向客户端的一致性模型专注于单个客户端的数据一致性，即单个客户端在多个副本上读写时不会出现数据不一致的问题

- 单调读：客户端读到数据 v1 后，之后读取到的数据不会旧于 v1
- 单调写：客户端在所有副本的写操作顺序一致
- 读你所写：客户端写入 v1 后，之后读的操作都不会旧于 v1
- PRAM：客户端在所有副本的操作顺序一致，但多个客户端之间的操作可以乱序
- 读后写：对于数据 x，若在写操作 w1 后读取到数据 v1，则之后的写操作 w2 必须基于 v1 或更新的数据，确保操作之间的因果关系

### 隔离级别

隔离级别定义了在系统中各个并发事务之间操作的可见性

隔离级别主要关注于各个并发事务之间的正确执行，而数据一致性则是关注于多个客户端之间关于已提交事务的数据之间的正确执行

### 异常

事务之间的隔离主要是为了避免并发事务之间由于可见性带来的异常

- 脏读：在事务 A 中读到了事务 B 【未提交的数据】
- 脏写：在事务 A 中进行的写操作覆盖了事务 B 中的【未提交数据】
- 幻读：事务 A 在读取一个范围内的数据时，由于事务 B 提交的新增，导致在第二次读取时出现第一次中没有的数据
- 不可重读：事务 A 中对同一份数据的两次读取会因为事务 B 对该数据的修改而产生差异
- 更新丢失：事务 A 与 事务 B 同时更新一份数据，后提交的数据会覆盖【先提交的数据】，导致其中一份更新丢失
- 读偏斜：偏斜指不符合数据一致性，如 X，Y 存在一致性约束 X + Y = 100， 事务 A 在读取数据 X = 50 时，事务 B 将其修改为 X = 30，Y = 70，此时事务 A 读取到 Y = 70，破坏了数据一致性
- 写偏斜：事务 A 与事务 B 在读取后同时修改一部分数据，但两个修改的部分结合起来时会破坏数据一致性，如 X + Y < 100 且 X = 10，Y = 20，事务 A 将 X = 70，Y = 20，而事务 B 将 X = 10，Y = 80，虽然两者都认为没有破坏一致性，但其结果为 X + Y = 70 + 80 > 100

隔离级别：

- 串行化：严格对读写操作加锁，将并发的操作变成串行
- 可重读：通过锁/MVCC保证一次事务中的数据可以重复读取到同一个值
- 快照：MVCC
- 读已提交：只能读取其他事务已经提交的数据
- 读未提交：可以读到其他事务未提交的数据

## 分布式共识

共识即是分布式系统中所有的节点都同意某个值的最终值

- 最终性：所有节点最终都能够确定一个值
- 协定性：最终确定的值被所有节点认可
- 正确性：最终确定的值是所有节点提出的值中的一个，不是凭空生成的

达成共识的意义：

- 互斥锁：解决多个节点之间的操作执行顺序，实现分布式锁
- 选举：在单主复制中，当主节点故障时，需要多数节点达成共识来选举新的主节点，避免脑裂
- 原子操作：需要所有节点达成共识来实现事务的原子性

状态机复制：对于相同的状态机，相同的输入会产生相同的输出，在分布式系统中的每个节点使用相同的状态机，使用类似日志的系统记录输入，如在一条顺序时间线上划分为多个索引，多数节点在一个索引上达成日志的共识(记录应该做什么)时将该行为视为已提交，通过状态机可以解决以下分布式系统难题：

- 在网络延迟、重排序、重发、丢包的情况下保证系统输出正常
  - 通过对已提交的行为达成共识，所有的状态机都只会执行已提交的行为，故所有状态机的输出一致，无论发包的状态如何
- 状态机不依赖时钟：
  - 通过任期号实现逻辑时钟
  - 各节点之间通过心跳信息来确认存活，并以此来确认是否需要选举新的领导节点，节点之间的系统时钟相差过大只会导致频繁的重新选举，不会影响整个系统的可用性与一致性
- 高可用：
  - 多数决
  - 重选举

### FLP 不可能理论

FLP 不可能理论指出由于网络延迟的存在，一个**完全异步**的分布式系统中只要**有一个节点出现故障**就**无法达成共识**，即无法同时满足安全性，活性，容错性

- 安全性：所有节点都达成共识
- 活性：节点在一定时间内会认同一个最终值
- 容错性：系统在部分节点出现故障后仍能正常运行

虽然无法在完全异步的系统中达成共识，但可以通过引入部分同步等方法绕过 FLP 不可能定理：

- 故障屏蔽：将发生故障的节点视为正常，一直等待其响应，同时故障节点可以通过持久化数据进行重启和恢复
- 故障检测器：节点之间可以通过故障检测器感知到故障节点并将其从系统中驱逐
  - 完美故障检测器：
    - 所有故障节点会被所有正常节点检测
    - 所有正常节点都不会被怀疑
  - 最终弱故障检测器：
    - 所有故障节点最终都会被部分正常节点检测
    - 所有正常节点在一段时间后都不会被怀疑
- 随机性算法：通过引入随机性算法，破坏 FLP 定理中的【无法指定确定性算法】部分，系统根据以高概率保证达成共识，而非严格确定性
  - 在 FLP 定理中，所有节点都使用一种确定性算法来计算确认值，即节点对固定的输入会产生固定的输出，而故障节点可以通过精心规划的输入来令其他节点一直在 2 - n 个可能的确认值之间摇摆，导致一直无法达成共识
    - 如 A，B 两个节点，在 t = 1 时， A = 1，B = 2；t = 2 时，A = 2，B = 1

  - 但引入随机性算法可以打破这种设计好的循环，所有节点最终都会达成共识(即使花费的时间不确定)


### Paxos 算法

Paxos 算法不是一个分布式系统，而是分布式系统通过执行一次 Paxos 算法(示例)来达成一次共识，它主要用于解决在分布式系统中出现故障节点时如何达成共识

Paxos 共识算法用于在多个节点中决议出一个最终值，它把节点分为四种角色：

- 客户端：向分布式系统发起请求
- 提议者：接收客户端请求，向投票者提出提议
- 投票者：多个投票者对提议进行投票，若半数投票者同意，则提议通过
- 学习者：学习者只读取提议值而不参与投票，当投票通过时会执行提议的操作

以无主复制为例，投票者可能会收到多个提议，每个节点选择接受的提议都可能不同：

- 节点接受第一个：如果所有节点都只取接收的第一个提议，则可能出现系统中没有过半数提议的情况，破坏了系统的活性(最终会得出一个确定值)
- 节点接受所有提议：如果节点接受收到的所有提议，则可能出现系统中存在多个过半数提议的情况，破坏了系统的安全性(得出同一个值)
- 二阶段协议：当提议 A 被系统批准后，参与提议 A 的节点此后不会再接受其他提议，由此保证系统只会批准一个提议。使用这种方式需要知道请求的顺序，在分布式系统中，通过组合递增的(请求 id，服务器 id)组合来标识请求的顺序
  - phase 1a：提议者收到客户端请求，生成新的提议编号并向多数的投票者广播
  - phase 1b：每个投票者收到提议编号后，比较其与自身持有的编号，若收到的编号更大，则记录该编号且承诺不再接受小于此编号的提议，若投票者本身已经持有提议值，则还需要将其一起返回，若收到的编号更小，则不予理会或返回拒绝
  - phase 2a：提议者收到来自多数投票者的通过响应，则向另一个多数节点广播提议编号与提议值，其中如果有来自 phase 1 的提议值，则直接使用，否则可以自己指定一个
  - phase 2b：投票者接收到提议值后，会先检查提议编号是否合法，再更新提议编号与提议值

活锁：当存在提议者 A 与 B，A 在发送 phase 1a 后还没发送 phase 2a 时，B 发送了编号值更大的 phase 1a，导致 A 的 phase 2a 中设置的共识值失效(选择编号更大的)，而在 B 发送 phase 2a 时 A 又发送 phase 1a，如此循环导致双方都无法成功设置值。可以通过设置超时来解决，即当提议者 A 在一段时间内无法获取到 phase 2b 的成功响应后，会进入休眠，让出机会来减小互相抢占的可能性

### Multi Paxos

Paxos 算法用于在系统中通过日志来复制状态机，日志中的一条记录即一次 Paxos 算法的运行结果，对于一组日志，可以将其视为一个数组 log[]，其中 log[i] 表示按顺序第 i 条日志的批准状态，每个节点都维护着自身的 log[]，即日志副本，一次 Paxos 实例的运行可以批准/确认一条日志，批准后的日志在所有节点的 log[] 中都被批准

当提议者收到来自客户端的请求时，会从 log[] 中找到最小的未被批准的日志记录，并尝试对其应用 Basic Paxos 来应用提议值，若改记录在其他节点中已被批准，则更改状态后再寻找下一个

由于每次选择的日志记录都需要进行一次 Paxos 算法，即两轮 RPC，造成大量的开销，故引入 Multi Paxos 提高效率

#### 领导者选举

为了解决因为多个提议者带来的冲突，通过选举出一个领导者负责提议者，在进行一次 prepare 阶段后就可以保证之后的提议编号都是递增，可以直接进入第二阶段

可以通过选择 server id 最大的节点作为领导者，即使存在两个领导者，也只是算法退化为 Basic Paxos

但如果领导者节点的日志远远落后，则需要较长时间进行同步，可能导致服务不可用

为了保证在并发时新提议的值与已接受的提议值相同，所有接受者仍需要在第一阶段返回已接受的提议值，且会在收到提议时检查之后的日志，若都为空，则返回 noMoreAccepted，当收到过半数返回时，领导节点认为不再需要进行 prepare 阶段

#### 日志副本完整性

由于状态机复制需要日志完全一致，所以要求所有节点的日志都一致，故存在以下问题：

- 需要同步接受者与领导者的日志，接受者可能因为网络原因还不知道某个日志的提议是否被批准
- 同步过程中，领导者只保证多数节点日志一致
- 重选领导者后，需要处理与前任领导者的差异

针对第一个问题，可以在接收者节点维护数组 acceptedProposal[] 来表名已被接受的提议，已接受的提议会将值设为无穷大(即不会被更大的提议所替换)，与表示最近未被批准的日志 firstUnChosenIndex，同时领导者也会维护一份自己的 firstUnChosenIndex

当进行第二阶段的 accept 时，领导者会将需要处理的日志编号与 leader.firstUnChosenIndex 发送给接受者，节点会对比其对应的日志 index 是否小于 leader.firstUnChosenIndex (即 (acceptedProposal[i] == leader.proposal (用于确认当前的领导者合法，即和第一阶段确认的相同)) && leader.firstUnChosenIndex > i)，若是则表示当前的日志已被批准，接受者需要修改其状态

为了保证所有节点(并非只有大多数)的日志同步，领导者在第一阶段后仍需要向未返回响应的阶段异步地发送同步消息，尝试将提议值复制给所有接受者

同时，为了处理前任领导者留下的差异，接受者在返回时也会附带 accepter.firstUnChosenIndex，当领导者发现其小于自身的 firstUnChosenIndex 时，也会异步地告知接受者更新 accepter.firstUnChosenIndex 所对应的日志状态，接受者在更新后会再次发送新的 firstUnChosenIndex，由此只到两者同步

#### 客户端请求

由于系统只存在一个提议者(领导者)，所以其他节点在收到客户端请求时会将其重定向至领导者，并将领导者地址返回给客户端，此后客户端会一直与领导者交互只到其不再响应

可能会出现领导者在执行完毕后还未返回响应时宕机的情况，此时客户端会认为系统未完成请求而重发，为了避免请求被多次执行，可以为其设置一个唯一的请求 id 并在成功执行后写入日志，当收到重复的 id 时会直接返回 success

#### 节点配置更改

系统中的节点配置会发生改变(ip，id，数量)，尤其是数量发生改变时，系统对多数的定义也会发生改变，可能会出现两个不相交的多数节点组，导致不同的值被批准

为了解决这个问题，可以将节点配置也作为日志进行共享，并且增加一个系统变量 a，表示经过 a 条记录后才开始使用新的配置，保证不会同时存在两个配置

### RAFT

RAFT 为了解决 Paxos 算法与工程化实现脱节的问题而提出，它也是基于单个领导者实现，用于通过日志复制来实现状态机复制

#### 基本概念

RAFT 中主要分为以下三种角色：

- 领导者：负责接收客户端请求，持久化与复制日志
- 跟随者：只能被动地响应领导者的消息
- 候选者：当跟随者在一定时间内未收到领导者的心跳信息时，跟随者会主动转换为候选者并向其他跟随着请求选票，若收到过半选票时转换为领导者，若收到其他领导者的心跳时则转换为跟随者，若选举无结果则立即开启下一次选举

RAFT 中使用任期处理时序问题，任期是每次选举产生的单调递增的序号，所有节点需要持久化当前的任期且不会接受低于当前任期的请求

RAFT 使用 索引 + 任期 唯一标识一条日志记录

#### 领导者选举

领导者在任期内需要持续地向跟随者广播自身的心跳信息来声明领导地位，若跟随者没收到来自领导者的心跳，则认为当前的领导者下线，会转换为候选者开启新一轮的选举，领导者的选举步骤如下：

- 跟随者转换为候选者，增加自身的任期数，给自身投票并向其他节点索要选票
- 当收到过半数的选票时，候选者会转换为领导者并广播自身的心跳
  - 产生网络分区时，若当前分区的节点数无法构成多数节点，则无法选出领导者

- 其他候选者收到心跳时会转换为跟随者并更新任期信息
- 如果当前选举没有产生领导者，则会立即开启下一轮选举

为了避免节点故障导致的选票重复/丢失，节点需要持久化自身的投票信息 voteFor

同时为了减少活锁(所有节点同时转换为候选者并给自身投票，导致永远无法选出领导者)，可以设置随机的选举超时时间，令各个节点在不同的时间开始选举

为了维护日志的一致性，RAFT 保证了以下两个特性：

- 当两个节点日志的索引与任期一致时，两者的日志内容可以视为一致
- 当日志索引 idx 提交时，idx 之前的所有日志都被提交，即 RAFT 中不允许出现日志空洞

为了实现该特性，节点需要持久化两个变量：

- prevLogIndex：上一个提交日志的索引
- prevLogTerm：上一个提交日志的任期

只有收到 prevLogIndex 与 preLogTerm 和节点最后一条日志的索引与任期相对应时，跟随者才会接受该记录，因为如果从初始状态开始一直维护这种对应关系，则可以保证这条日志之前的日志都是一致的，这里保证了**如果一条记录在上一任领导者中已提交，那么这条记录在之后的所有领导者中都已提交**

#### 领导者更替

当领导者宕机或网络故障时，需要重新选举领导者，对于此时产生的日志不一致问题，RAFT 选择不做处理而直接令所有跟随者的日志与新任领导者同步

为此需要保证领导者的日志正确性：

- 领导者的已提交的日志不会被覆盖
- 对于未提交的日志，需要延迟提交，直到确认该日志安全后才能提交

因为需要新任领导者的包含足够多的已提交日志，所以在索要选票时新增 prevLogTerm 和 prevLogIndex 来告知当前候选者最新的日志任期与索引，如果其小于投票者自身，则拒绝为其投票，由此选举出的领导者一定是在多数节点中任期最新的，即最有可能包含所有已提交日志的

选举出的领导者更为权威，不必去追赶其他节点的日志进度而造成系统堵塞

#### 延迟提交

为了保证日志的安全性，除了修改选举方法，还需要保证提交的日志满足以下条件：

- 日志需要写入多数节点
- 领导者需要在多数节点中至少写入一条自身任期的日志

只在多数节点写入的日志是不安全的，因为可能存在一个节点由于网络分区而在日志索引领先其他节点，若它当选领导者时，可能会覆盖掉其余节点已提交的日志

领导者在多数节点中都写入一条自身任期的日志可以避免过时/脑裂的领导者提交冲突的日志

#### 实现线性一致性

问题：出现网络分区时，整个系统会出现两个领导者，且旧领导者无法感知到有新领导者的出现，如果此时客户端向旧领导者发送读请求，则可能会读取到旧数据，不满足线性一致性中所有读操作都应读取最近一次写操作结果的要求

解决：

- 确保领导者的正确性：
  - 可以在每次读请求时也像写请求一样，将读操作广播至所有从节点并提交，确保当前领导者的正确性，但每次读请求都需要一次完整的 RAFT 操作，开销较大
  - 领导者维护 readIndex，表示可以正确执行读操作的索引位置
    1. 领导者需要等待在当前任期内存在已提交的日志(no-op)，由此确认自身的正确性
    2. 领导者将 commitIndex 赋值给 readIndex
    3. 领导者收到读请求后，向所有从节点发送心跳，确认自身仍是合法的领导者
    4. 领导者等待状态机至少执行至 readIndex 后处理读请求
    5. 跟随者通过向领导者同步 readIndex 来确保能够正确处理读请求

### 配置变更

在 Raft 中通过二阶段提交确保配置变更不会影响领导者选举，导致出现在新旧配置下的两个领导者：

1. 向领导者写入 C_oldAndNew(包含新旧两种配置的所有节点)，且配置立刻生效
2. 领导者通过 AppendEntries 将 C_oldAndNew 传播给多数跟随者
3. 跟随者收到配置后立刻生效，领导者提交 C_oldAndNew，此时系统中同时包含新旧两种配置的所有节点，此时的领导者选举必须同时满足新旧两种配置的多数节点
   - 当旧领导者属于旧配置时，
4. 向领导者写入 C_new 配置，且立刻生效
5. 领导者通过 AppendEntries 将 C_new 传播给多数跟随者
6. 跟随者收到配置后立刻生效，领导者提交 C_new，此时系统转为新的配置
   - 不属于新配置的旧领导者会继续服务一段时间，直到新配置提交后下台

#### 扰乱集群

不属于新配置的节点如果没有关闭，则会因为不再收到新领导者的心跳而开启新一轮的选举，旧节点会将选举请求发送到新的领导者，由于其任期号更大，新领导节点会转为跟随者，但因为旧节点的日志索引落后而无法成为领导者，所以旧节点虽然不会当选，但可能一直影响新配置中的领导者选举

preVote：跟随者在发送选举请求前，会先发送 preVote 请求，其校验的信息与选举请求一样，当未收到多数节点的投票时即可确认自己无法当选，从而不会开始真正的选举，以此避免影响领导者选举。因为旧节点不处于新配置中，所以它的日志索引总是落后的，但为了避免新加入的节点在同步日志前收到旧节点的 preVote 而同意，还需要规定所有节点在收到来自领导者的心跳信息后的一次选举超时时间内会拒绝 preVote 请求(领导者在选举超时前不需要再次选举)

### 活性问题

当出现网络连接问题导致当前领导者只能与一部分节点通信，此时另一部分会因为无法收到领导者的心跳而开始下一轮选举，但在 preVote 阶段会由于还与旧领导者通信的节点的拒绝(收到来自领导者的心跳信息)而无法获得多数选票，使得无法产生新的领导者，由此一直循环导致系统不可用

因为 preVote 的限制导致系统可容忍的故障节点数变少，故需要引入 checkQuorum 机制，即领导者在收不到多数节点的响应时自动下台

### 日志压缩

随着系统运行时间变长，节点积累的日志数据也会变大，过多的日志数据会导致占用大量的磁盘/内存空间，且影响节点重启恢复的时间，故需要对节点存储的日志进行压缩

日志的压缩可以通过舍弃过期的提交，例如在日志 x=3 之前的日志 x=2 就是过期的

压缩日志的实现有以下共同点：

- 压缩日志的行为由各个节点自己负责，这样可以提高系统的模块化，也避免领导者将节点已有的日志重复发送
- Raft 算法要求领导者保留舍弃日志的最后一条(appendEntries)与配置信息(避免配置变更失败后无法回滚到原来的配置)
- 领导者在舍弃日志之后要保证在重启时先加载日志快照再接受客户端请求，并且需要向进度滞后的跟随者发送更新的快照

## 分布式事务

分布式事务同数据库的事务一样，也需要实现 ACID，但由于一致性是需要与应用程序配合实现，故分布式事务不讨论一致性

对于持久性，可以将数据存储在持久化的存储设备，这里和单点事务区别不大

### 原子性

保证原子性需要兼顾硬件与软件，即软件不能出现异常，硬件在写入数据时也不能故障，在单点事务中可以通过 WAL(Write Ahead Log) 提前写入日志，当出现故障后可以通过日志来继续完成事务或回滚

但在分布式系统中，需要确保操作在每个节点都成功/失败，所以分布式事务的原子性是由原子提交协议(ACP)实现

### 原子提交协议

ACP 的三个特性：

- 协定性：所有节点最终都会得出同一个值，即都提交/拒绝
- 有效性：当没有故障发生时，所有节点都会同意提交，只要有一个节点拒绝，那么无法提交事务
- 终止性：所有节点都会得出一个值
  - 弱终止性：当没有故障发生时，所有节点都会得出一个值
  - 强终止性：没有故障的节点一定会得出一个值

#### 二阶段提交

为节点引入协调者和参与者两种角色：

- 协调者：负责统筹各个参与者的事务执行状态，以此来确定是否提交
- 参与者：实际执行事务操作的节点

两阶段提交的流程如下：

1. 协调者将事务操作请求发送给所有的参与者
2. 参与者收到请求后开启事务，准备执行操作所需的条件与资源，在执行完毕后返回执行结果(是/否)
3. 协调者收到所有参与者的响应
   1. 全部为是时，通知所有参与者可以提交事务
   2. 存在为否的响应时，通知所有参与者回滚
4. 参与者收到指令后，清理占用的资源，记录提交日志或通过日志进行回滚，完成后返回响应
5. 协调者收到响应后结束事务

由于网络问题或节点故障，二阶段提交可能出现以下问题：

- 协调者无法收到参与者的事务执行情况，导致无法确定事务能否提交
  - 引入超时机制，响应超时即视为失败
- 参与者返回响应后，协调者宕机，参与者无法确认当前事务的执行情况，故一直占用事务执行的资源(锁)，导致系统被阻塞
- 协调者在发送事务提交请求给部分参与者后，协调者和参与者都宕机，新的协调者无法得知之前的事务提交情况，如果强制回滚，在参与者恢复后可能导致数据不一致，如果强制提交，则可能前任协调者的回滚决定冲突

#### 三阶段提交

二阶段提交在协调者宕机后会阻塞参与者，因此是一个弱终止性的实现，三阶段提交通过增加一个预提交过程，将所有节点的执行情况发送给所有节点，由此在协调者宕机后可以选举出新的协调者，从而实现强终止性

三阶段提交流程如下：

- 准备阶段：
  1. 协调者发送准备消息给所有参与者
  2. 参与者检查能否满足执行事务的条件与资源，并以此返回是/否
  3. 若协调者收到否的响应，则直接终止事务
- 预提交阶段：
  1. 协调者向参与者发送检查并执行事务的请求
  2. 参与者根据事务执行情况返回
- 提交阶段：
  1. 若协调者收到否的响应，则通知参与者回滚，否则提交
  2. 参与者执行后返回响应
  3. 协调者终止事务

三阶段提交虽然实现了强终止性，但牺牲了可用性：

1. 在准备阶段后，协调者发送了部分预提交消息后宕机
2. 此时发生网络分区，恰好将参与者按收没收到预提交消息分为两个分区
3. 两个分区都会选举出新的协调者，收到消息的分区会继续提交事务，而没收到的则不会执行，甚至直接中止事务

#### Paxos 提交

因为分布式事务可以看作分布式共识的一个子问题，即节点就是否提交达成共识，所以可以结合 Paxos 算法判断是否提交事务

在 Paxos 提交中有三种角色：

- 资源管理者：每个资源管理者都是一个 Paxos 实例的提案者，它负责发起事务与创建 Paxos 提案，并在领导者和接受者中运行 Paxos 算法决定是否提交
- 领导者：类似 Multi Paxos 的领导者，负责协调整个集群的 Paxos 算法，并且在宕机后可以通过选举产生新的领导者
- 接受者：与资源管理者共同构成 Paxos 实例，负责投票决议出是否提交事务，所有资源管理者共享所有接受者

Paxos 提交流程如下：

1. 一个资源管理者决定提交事务后，向领导者发送 BeginCommit 消息
2. 领导者收到后向所有资源管理者发送 Prepare 消息
3. 资源管理者收到后，先判断自身是否满足事务提交的条件，若满足则向所有接受者发送带有提案编号和 Prepare 的消息；若不满足，则直接向领导者发送 Abort 消息
4. 接受者收到提案后，如果提案值不小于自身的提案值，则接受该提案并向领导者返回带有提案编号和值的消息
5. 领导者收到多数接受者的提案值后认定 Paxos 实例的多数派达成，该资源管理者选定了决议值，在获取到所有资源管理者的决议值后，如果全为同意则通知所有资源管理者提交事务，否则向所有的资源管理者发送 Abort 消息，通知他们中止事务

Paxos 的优化：

- 由于所有资源管理者共享所有接受者，所以在执行 Paxos 实例时需要大量节点，可以通过将接受者与资源管理者部署在同一台机器上，此时本地的通信延迟就可以忽略不计
- 接受者可以批量地将多个资源管理者的提案值返回给领导者
- 可以不在领导者处汇总提案值，而是直接返回至对应的资源管理者

Paxos 提交实际上是在二阶段提交的基础上引入了多个协调者以实现容错

#### Quorum 提交

在三阶段提交的基础上，引入 Quorum 机制解决在预提交阶段出现网络分区导致的不可用

Quorum 机制规定：在 V 个节点中，至少存在 Vc 个节点同意提交或至少存在 Va 个节点不同意时才得出结论，且 Vc + Va > V，由此限制了不可能同时提交和中止

Quorum 协议包含三个子协议：

- 提交协议：类似三阶段提交的预提交阶段，但协调者需要收到至少 Vc 个节点的同意才会继续提交阶段
- 中止协议：当在预提交阶段出现网络分区时，每个分区会选举出新的协调者，协调者会收集分区内的节点信息
  - 当存在节点处于已提交/已中止状态，协调者会令其继续进行提交/中止
  - 至少存在一个节点处于预提交状态(即需要执行提交阶段)，且至少 Vc 个节点等待同意提交的投票结果(满足 Quorum 机制)，协调者会发送预提交信息，并在收到 Vc 个同意投票后发送真正的提交消息
  - 不存在节点处于准备提交阶段(即收到旧协调者的提交消息)，且至少 Va 个节点等待中止的投票结果，协调者会先发送预中止消息，在收到 Va 个响应后发送真正的中止消息
- 合并协议：在分区结束后将所有的代理协调者合并，再选举出新的协调者

### Saga 事务

Saga 事务将一个事务划分为多个小的子事务，且每个子事务都有其对应的补偿事务用于回滚，由此来降低执行事务时占用资源对系统的影响

例如对于事务 T，可以划分为 T1，T2，T3 等子事务，同时搭配 C1，C2，C3 等补偿事务，当执行 T2 事务需要中止时，可以调用 C1 来对 T1 进行回滚

Saga 事务是对性能和可用性之间的一种妥协，但其无法保证隔离性

### 隔离性

并发控制：

- 悲观锁
- 乐观锁
- 多版本并发控制(MVCC)

#### 两阶段锁

将锁分为两种：

- 读锁：在进行读操作时上锁，一个资源可以加多个读锁，但读锁会阻塞写锁的获取
- 写锁：在进行写操作时上锁，写锁作为排他锁，在获取前后都不允许有其他锁

两阶段锁将一个事务分为两个阶段：

- 扩张阶段：只允许上锁
- 收缩阶段：只允许解锁

通过严格地控制加锁与解锁的时机，实现事务的串行化

两阶段锁根据收缩阶段的时机还有两种子实现：

- 严格两阶段锁：收缩阶段发生在事务提交后，保证事务执行期间的数据不会被其他事务读取
- 强严格两阶段锁：收缩阶段发生在事务结束后

由于使用排他锁，两阶段锁需要处理出现死锁的风险：

- 死锁避免
- 死锁预防
- 死锁检测

#### 乐观并发控制

乐观并发控制认为事务之间产生冲突的可能性较小，只需要在事务提交时做校验即可，从而避免了锁带来的问题

基于校验的乐观锁分为三个阶段：

- 读取阶段：获取需要数据的副本，后续的修改都在这一份私有副本上进行
- 校验阶段：检查原始数据是否有产生修改，如果有变则中止当前事务
- 写入阶段：没有冲突发生，将副本的修改应用到原始数据中

基于时间戳的乐观锁：

- 记录当前事务的开始时间 TS(Start)，数据的写入时间 TS(Write) 和读取时间 TS(Read)
- 对于读操作，当 TS(Start) < TS(Write) 时，表示事务读取到了过期的数据，则中止
- 对于写操作，当 TS(Start) < TS(Read) || TS(Start) < TS(Write) 时，表示在事务可能在修改过期的数据

乐观锁虽然没有死锁问题，但可能导致不可逆的修改：

1. 事务 T1 和 T2 读取并修改同一份数据 X
2. T1 在 T2 修改 X 后开始读取并修改 X
3. T1 提交后，T2 被中止回滚，导致 T1 提交了一个基于被回滚的值产生的修改且无法恢复

并且在分布式中，各个节点之间的时间戳可能存在较大的差异

#### 多版本并发控制

通过为每份数据创建多个版本的副本来实现并发控制，事务读取数据的特定副本，提交时再创建新的副本，这样写操作不会阻塞读操作

多版本并发控制提供一种基于快照的隔离级别，可以与二阶段提交、乐观并发控制相结合，无论具体实现如何，MVCC 的数据副本都需要包含元数据来控制版本：

- txn-id：表示持有当前版本数据写锁的事务时间戳 Ti，若没有事务持有则为 0，表示当前版本可以安全访问
- begin-ts：创建该副本的事务的提交时间戳
- end-ts：最新版本的 end-ts 为 INF，否则为上一个版本的 begin-ts

begin-ts 和 end-ts 共同表明了一个版本的生命周期

##### 多版本二阶段锁

在元数据中增加 read-cnt 表示当前版本的读锁数量，其执行流程如下：

1. 事务开始时获取分配的时间戳 Ti，并以此获取相应版本的数据(begin-ts <= Ti && Ti < end-ts，表示事务可以读取的最新版本数据)
2. 判断当前版本是否有写锁(txn-id == 0)，若不为零，即有其他事务正在修改，则中止事务，否则将 read-cnt++
3. 创建新的数据版本，txn-id = Ti，begin-ts = 0，end-ts = 0，在此版本上进行修改
4. 提交版本时先判断所有读取的数据版本是否被修改，再将新版本的 txn-id = 0，begin-ts = Tc，end-ts = INF，并将上一个版本的 end-ts = Tc，txn-id = 0

##### 多版本乐观并发控制

由于数据版本本身就是一个副本，所以对于基于校验的多版本并发控制，私有副本就不再需要了，其流程也分为三个阶段：

- 读取阶段：对于读操作，找到事务可读取的最新版本，判断是否有加写锁；对于写操作，判断数据的最新版本是否有加写锁
- 校验阶段：遍历读取的所有数据版本，判断是否被修改过
- 写入阶段：将修改后的最新版本提交

##### 多版本时间戳并发控制

在版本的元数据中加入 read-ts 表示上一个读取该版本的事务，其流程如下：

1. 对于事务 Ti，其读操作会找到事务可读取的最新版本，判断是否有加写锁，再判断 read-ts 的大小，将其修改为更新的一个
2. 在写入前，会判断当前最新版本的 read-ts <= Ti，否则表示有其他事务读取到了过期的数据，需要中止当前事务
3. 提交修改后的最新版本

##### 版本存储

每个数据的版本在存储时对元数据有三种实现方式：

- 仅追加(Append Only)：将最新版本与历史版本存储在同一份表中，通过一个指针列来标识下一个版本所在行，根据遍历的方向分为老到新和新到老
  - O2N：访问指针指向最老的版本，每个标识指针指向更新一个版本的数据，这种实现在插入新版本时不需要移动访问指针，但在获取新版本时需要遍历所有版本
  - N2O：访问指针指向最新的版本，因为读取数据时大多数是获取最新版本，所以对读操作更友好，但写入时需要处理访问指针的变更
- 时间旅行表：将最新版本与历史版本分开存储，最新版本会存储指向历史版本的指针
- 增量存储：只存储每次变更的部分，类似日志存储，可以避免大量的冗余数据(如只修改其中一小部分数据也会导致新增一份完整的数据副本)，但每次读取都需要执行所有的历史记录

##### 垃圾回收

为了避免历史版本无限制增长，对于过期的版本有两种清理方式：

- 元数据级别：当一个历史版本的 end-ts 小于当前所有活跃的事务 Ti(即该版本不可能再被读取)，将其视为过期版本进行清理
  - 后台清理：由专门的后台线程进行检测与清理
  - 协同清理：在事务执行时检测，但无法清理不参与事务的数据
- 事务级别：当一个事务创建的版本不再被活跃的事务读取时，将其视为过期，并根据其读写集进行清理，需要额外维护每个事务的读写集

#### Percolator

基于 Bigtable(只允许单行事务)实现的快照隔离级别，使用以下元数据：

- write：事务提交时间戳
- lock：锁信息
- data：数据

Percolator 对于一次事务的处理流程如下：

1. 分配事务开始时间戳 start_ts
2. 将事务中的所有写操作缓存起来，直到提交时一并写入
3. 预写阶段：二阶段提交的第一阶段，从所有写操作中选取一个主锁，其他写操作作为次锁，主锁用于锁住事务中写操作设计的所有数据
   1. 启动一个 Bigtable 单行事务
   2. 读取写操作涉及的行的 write 元数据信息，检测是否有其他事务在当前事务开始后修改
   3. 读取 lock 元数据信息，检测是否有其他事务持有该行的锁
   4. 事务开始将更新数据写入新的数据行
   5. 获取新行的锁，以 start_ts 作为 Bigtable 的时间戳，以主锁的 {primary.row, primary.col} 作为值写入 lock 列
4. 提交阶段：
   1. 获取提交时间戳
   2. 对主锁涉及的行启动单行事务，检查事务是否持有 lock 列的锁
   3. 若事务仍持有锁，以提交时间作为 Bigtable 的时间戳，以开始时间作为 write 列的值更新数据，使该数据对其他事务可见
   4. 释放事务持有的主锁，检查事务主锁的写操作是否对其他事务可见
   5. 主锁的写操作提交后表示事务已完成，异步执行次锁的写操作

对于读操作，先检查时间戳在 [0, start_ts] 内是否有其他事务持有该行的锁，若存在冲突的锁，则读事务会等待锁被释放，之后读取 [0, start_ts] 内的所有版本，判断能否找到最新版本

## 时间和事件顺序

### 时钟同步

NTP 是一个时钟同步协议，客户端通过向 NTP 服务发送请求以计算并纠正时间偏差：

1. 客户端记录发送请求的时间 t0，向 NTP 服务器发送请求
2. NTP 服务器记录收到请求的时间 t1
3. NTP 服务器记录发送响应的时间 t2
4. 客户端记录收到响应的时间 t3，计算请求的延迟时间 θ = (t3 - t0) - (t2 - t1)，假定请求和响应的延迟相同，则当前客户端的时间应为 t2 + θ / 2

客户端会同时请求多个 NTP 服务器并筛除其中的异常值，从其中最好的三个候选值中得出当前时间

因为时钟同步可能会导致客户端时间提前/延后，在程序需要计算时间间隔时(例如计算某个函数的耗时)，时钟同步将当前时间回调，这就可能会导致开始时间晚于结束时间，即计算出的时间间隔为负数

可以通过操作系统提供的单调时钟(nanoTime)处理，单调时钟不受 NTP 服务器影响，它会以某个时间点为起点(程序开始执行)，然后计算函数的执行时长，以严格保证时间单调增长。但是单调时钟只适用于单机系统，在分布式系统中，由于各个节点的起点不同，得出来的单调时间就可能不一致

### 逻辑时钟

既然实现现实中的时钟同步较为复杂，那么可以退而求其次，实现一种基于因果关系的逻辑时钟：

- 同一个进程中，a 事件先发生于 b 事件，则有 a -> b
- 因为发送消息的行为 a 必定先于接受消息的行为 b，所以必然可以得出 a -> b
- a -> b -> c 可以得出 a -> c
- 当 a -> b 不成立时，认为 a 与 b 时并行的(a || b)

在逻辑时钟里，每个进程都维护自身的逻辑时钟(初始为 0)：

- 每当在本进程执行一个事件时，时钟 + 1
- 当发送消息至其他线程时，现将本地时钟 + 1，再把时钟号与请求消息一并发送
- 接受消息的进程会将自身的时钟设置为 max(请求时钟号, 自身时钟号) + 1

逻辑时钟仅表示一种逻辑上的顺序，而非严格遵循一条时间线上的顺序，所以即使 a 在时间线上先于 b 发生，但只要其逻辑时钟的编号大于 b 就仍然认为 b 先于 a 发生。

在只使用逻辑时钟时得到的是一个局部的偏序关系，即无法确认时钟号相同的两个事件之间的先后关系，所以可以通过引进新的机制来实现全局的全序关系，例如进程优先级

假设有两个进程 Pi 与 Pj，当 i < j 时，我们认为 Pi < Pj，则对于两个线程上的事件 Ei 和 Ej 有：

- 当 Ci != Cj 时，两者的顺序由时钟编号决定
- 当 Ci == Cj 时，因为 Pi < Pj，所以 Ei 先于 Ej 发生 

全序关系可以应用于分布式锁时，当系统的分布式锁有以下规定：

- 资源不可共享
- 获取资源的顺序严格按照发送请求的顺序
- 占用资源的进程最终都会释放资源

如果使用一个中心化的资源分配器实现，则可能出现以下情况：

- 进程 a 先于 b 发送获取资源的请求
- 由于网络延迟，b 的请求先于 a 到达，导致 b 先得到资源，违反了规定 2

使用全序关系可以实现一种去中心化的分布式锁：

- 所有进程都维护一份自己的请求记录表，用于记录所有进程请求资源的顺序，表中数据的格式为 (时间戳 : 进程号)
- 当进程 a 需要请求资源时，先向所有进程广播这次请求，写入 (Ta : Pa)；之后 b 也广播 (Tb : Pb) 且 Tb < Ta
- 其他进程收到广播请求后，会记录 Qa 与 Qb，并将 Qb 置于表头部，再向 a 和 b 发送 ACK 响应
- a 收到所有的 ACK 响应后，检查自身的记录表，发现表头部是 Qb，所以 b 能获取资源而 a 不能
- 在 b 释放资源后，将 (Tb: Pb) 从表头中移除并广播移除操作，此时 a 可以获取资源

这种分布式锁可以类比 Paxos 和 Raft 中的状态机，只要记录表的内容相同(逻辑时间相同)，进程的执行结果就相同

### 向量时钟

因为逻辑时钟只维护当前进程本身的时间戳，所以还需要借助进程优先级来确定事件的先后顺序，因为 Ta < Tb 并不能推导出 Ea < Eb

向量时钟在逻辑时钟的基础上进行改良，每个进程会通过一个向量维护所有进程的时间戳：

- 对于 N 个进程，每个进程维护一个 N 维向量，每个向量的初始值为 0，令进程 i 在自身的向量中表示为 C_i_i
- 当 i 执行一个事件时，将 C_i_i + 1
- 当 i 向 j 发送请求时，先将 C_i_i + 1，j 收到请求后，对向量中的所有值执行 C_j_k = max(C_i_k, C_j_k)

当满足以下条件时，a -> b：

- 对于所有 k，都有 C_a_k <= C_b_k
- 至少存在一个 k，令 C_a_k < C_b_k

向量时钟仍然只能得出事件的偏序关系

版本管理是向量时钟的一种改进：

- 它只会在数据更新时修改时间戳，而不是每次发送和接受时都修改
- 作为发送方的进程也需要更新向量，而不是只有接收方更新

### 分布式快照

在分布式系统中，需要存储全局快照以进行数据分析、故障排查、异常回滚等操作，因此需要在分布式系统中实现能够在不阻塞系统的前提下生成可用快照的算法

Chandy-Lamport 算法认为节点的状态改变是由消息的发送和接受引起的，所以要生成一个节点的局部快照需要记录节点的状态以及其收到的消息。它将整个系统分为存储数据的节点和传播消息的通道，其具体的执行流程如下：

1. 选择一个起始进程，记录其节点状态，通过节点的所有输出通道传播一条 marker 消息
2. 在通道 i 接收到 marker 消息的节点也会记录自身节点的状态，通过所有输出通道传播 marker 消息，监听并记录除了 i 以外的所有输入通道
3. 当进程在其他的所有输入通道都收到 marker 消息后，当前节点的局部快照就生成了
4. 直到所有进程都收到来自其所有输入通道的 marker 消息后，控制服务器会收集所有的局部快照并组装成一个全局快照