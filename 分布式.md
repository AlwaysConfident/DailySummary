# 分布式

## CAP 理论

- Consistency：一致性，所有节点访问同一份最新的数据副本
- Availablility：可用性，非故障的节点在合理的时间内返回合理的响应(非错误或超时的响应)
- Partition Tolerance：分区容错性，分布式系统出现网络分区时，仍然能够对外提供服务

![](https://oss.javaguide.cn/2020-11/cap.png)

CAP 定理指出对于一个分布式系统，当设计读写操作时，只能同时满足三点中的两个，但由于分布式系统理论上不可能选择 CA 架构，只能选择 CP 或 AP 架构，即所谓 3 选 2 实际上是 2 选 1，P 是必须满足的

- CP：ZooKeeper 保证 CP，任何时刻对 ZooKeeper 的请求结果都是一致的，但会由于 Leader 选举或半数以上节点不可用导致无法提供服务
- AP：Eureka 保证 AP，Eureka 中没有 Leader，所有节点都是平等的，故没有 Leader 选举，且只需要存在一个正常的节点就可以提供服务，但不能保证该节点上的数据是最新的

### 不可能为 CA 架构

若系统出现多个网络分区，系统中的某个节点正在进行写操作，为保证 C，必须禁止其他节点的读写操作，这样就和 A 发生冲突

如果为了保证 A，允许其他节点正常读写，就和 C 发生冲突

但在正常情况，没有网络分区出现，可以同时保证 C 与 A

### 网络分区

原本联通的节点由于某些故障导致彼此分隔，由此产生了多个隔离的区域，即网络分区

![](https://oss.javaguide.cn/2020-11/partition-tolerance.png)

## BASE 理论

BASE 即：

- Basically Available：基本可用，分布式系统在出现不可预知故障的时候，允许损失部分可用性，但系统整体仍可用
  - 允许损失部分可用性：
    - 响应时间上的损失：处理用户请求的时间变长
    - 系统功能上的损失：部分非核心功能不可用
- Soft-state：软状态，允许系统中的数据存在中间状态(CAP 中为数据不一致)，并认为该中间状态的存在不会影响系统的整体可用性，**即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时**
- Eventually Consistent：最终一致性，强调系统中的所有数据副本，在经过一段时间的同步后，最终会达到一个一致的状态，因此，最终一致性只要求系统保证最终数据能够达到一致，而不需要实时保证数据的强一致性
  - 一致性级别：
    - 强一致性：系统写入什么就读出什么
    - 弱一致性：不一定可以读出最新值，也不保证经过一段时间后能读到最新值，只会尽量保证某个时刻达到数据一致性的状态
    - 最终一致性：系统保证一段时间内达到数据一致的状态
  - 实现最终一致性：
    - 读时修复：在读取数据时，检测数据的不一致，进行修复，当查询数据时检测到不同节点的数据副本不一致，系统就自动进行修复
    - 写时修复：在写入数据时，检测数据的不一致，进行修复，当节点之间远程写数据时，若写失败就将数据缓存，然后定时重传，修复数据的不一致性，性能消耗较低(写操作一般较少)
    - 异步修复：通过定时对账检测副本数据的一致性，并修复

BASE 理论是对 CAP 中一致性 C 和可用性 A 权衡的结果，大大降低了对系统的要求

![](https://imgconvert.csdnimg.cn/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC81LzI0LzE2MzkxNDgwNmQ5ZTE1YzY?x-oss-process=image/format,png)

### 核心思想

即使无法做到强一致性，每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性，也就是一致性换高可用性，系统中一部分数据不可用/不一致时，整个系统"主要可用"

BASE 理论本质上是对 CAP 的延伸和补充，更具体地说是对 AP 方案的补充

在 CAP 中，当发生网络分区时，AP 方案会暂时放弃一致性，在分区故障修复后应该达到最终一致性，这也是 BASE 理论进行扩展的地方

## Paxos 算法

第一个被证明完备的分布式系统共识算法(前提是不存在恶意节点)，共识算法的作用是让分布式系统中的多个节点之间对某个提案达成一致的看法，像哪个节点是 Leader 节点，多个事件发生的顺序等等都可以是一个提案

Paxos 算法主要包含两个部分：

- Basic Paxos 算法：描述多节点之间如何就某个值(提案 value)达成共识
- Multi-Paxos 思想：藐视执行多个 Basic Paxos 实例，就一系列值达成共识，即执行多次 Basic Paxos，核心还是 Basic Paxos

基于 Paxos 算法，衍生了多种简化版本：

- Raft 算法
- ZAB 协议
- Fast Paxos 算法

### Basic Paxos 算法

Basic Paxos 中存在 3 个重要的角色：

- 提议者/协调者：负责接收客户端请求并发起提案，提案信息通常包括提案编号和提案值
- 接收者/投票员：负责对提议者的提案进行投票，同时需要记住自己的投票历史
- 学习者：若有超过半数接收者就某个提议达成共识，学习者就需要接收这个提案，并就该提案作出运算，将运算结果返回给客户端

即 发起提案 → 提议者，进行投票 → 投票员，执行提案 → 学习者

![](https://oss.javaguide.cn/github/javaguide/distributed-system/protocol/up-890fa3212e8bf72886a595a34654918486c.png)

为了减少实现算法所需的节点数，一个节点可以拥有多个角色，并且，一个提案被选定需要被半数以上的投票员接受，这样 Baisc Paxos 算法还具备容错性，在少于一半节点出现故障时，集群仍能正常工作

### Multi Paxos 思想

Basic Paxos 算法仅能就单个值达成共识，为了能够**对一系列的值达成共识**，需要用到 MultiPaxos

Multi Paxos 只是一种思想，核心是通过多个 Basic Paxos 实例就一系列值达成共识

Raft 算法即 Multi Paxos 的一种简化实现

## Raft 算法

共识算法用于应对高度动态的环境，该环境根据需求进行扩展和收缩，且对故障快速作出反应并自动适应

### 拜占庭将军

假设多为拜占庭将军中没有叛徒，信使的信息可靠但可能被暗杀，此时将军们如何达成是否进攻的一致性决定？

解决方案大致为：先在所有将军中选出一个大将军(Leader)，用来做出所有决定

举例如下：假如现在一共有 3 个将军 A，B 和 C，每个将军都有一个随机时间的倒计时器，倒计时一结束，这个将军就把自己当成大将军候选人，然后派信使传递选举投票的信息给将军 B 和 C，如果将军 B 和 C 还没有把自己当作候选人（自己的倒计时还没有结束），并且没有把选举票投给其他人，它们就会把票投给将军 A，信使回到将军 A 时，将军 A 知道自己收到了足够的票数，成为大将军。在有了大将军之后，是否需要进攻就由大将军 A 决定，然后再去派信使通知另外两个将军，自己已经成为了大将军。如果一段时间还没收到将军 B 和 C 的回复（信使可能会被暗杀），那就再重派一个信使，直到收到回复。

### 共识算法

共识是可容错系统中的一个基本问题：即使面对故障，服务器也可以在共享状态上达成一致

共识算法允许一组节点像一个整体一样工作，即使其中一些节点出现故障，整体也能继续工作，其正确性主要源于复制状态机的性质：一组 Server 状态机计算相同状态的副本，即使有一部分的 Server 状态机宕机，仍能继续进行

![](https://oss.javaguide.cn/github/javaguide/paxos-rsm-architecture.png)

复制状态机的实现主要基于复制日志，当客户端请求到达服务器时，生成一份复制日志保留其执行命令与顺序，每个状态机都有一份复制日志，其中的命令与顺序一致，由此执行的结果也是一致的

因此共识算法的工作就是保证复制日志的一致性，服务器与其他服务器上的共识模块通信，以确保即使某些服务器故障，其他服务器也能完成工作

每个日志最终包含相同顺序的请求，一旦命令被正确地复制，就被视为已提交，每个服务器的状态机按照日志顺序处理已提交的命令，并将输出返回给客户端，由此形成一个单一、可靠的状态机

#### 共识算法的特性

- 安全：确保在非拜占庭条件(没有恶意节点)下的安全性，包括网络延迟、分区、包丢失、复制、重排
- 高可用：只要大多数服务器正常运作且可以相互通信，那么整体可以看作完全功能可用，发生故障的服务器也可以在修复后从稳定存储上的状态中恢复并重新加入集群
- 一致性不依赖时序：错误的时钟和极端的消息延迟，在最坏的情况下也只会造成可用性问题，而不会产生一致性问题
- 不需要全部服务器响应：集群中大多数服务器响应即可完成命令，不需要等待运行缓慢的服务器，提高性能

### Raft 基础

#### 节点类型

Raft 基于 Paxos 算法，其包含的三种节点也类似：

- Leader：负责发起心跳，响应客户端，创建日志，同步日志
- Candidate：Leader 选举过程中产生的临时角色，由 Follower 转换而来，负责发起投票参与竞选
- Follower：接受 Leader 的心跳和日志同步数据，投票给 Candidate

正常情况下，只有一个服务器是 Leader，其余都是 Follower，Follower 是被动的，不会发送任何请求，只响应来自 Leader 和 Candidate 的请求

![](https://oss.javaguide.cn/github/javaguide/paxos-server-state.png)

#### 任期

raft 算法将时间划分为任意长度的任期(term)，每个任期开始都是一次选举，在选举开始时，若干个 Candidate 会尝试成为 Leader，赢得选举的 Candidate 会成为任期内的 Leader，没有选出则开始下一个任期并立刻进行下一次选举，raft 算法保证任期内最少有一个 Leader

![](https://oss.javaguide.cn/github/javaguide/paxos-term.png)

每个节点都会存储当前的 term 号，当服务器之间进行通信时会交换当前的 term 号，如果有服务器发现自身的 term 号更小，那么它会更新到较大的值，如果 Candidate/Leader 的 term 过期，则立即退回为 Follower，服务器会拒绝过期 term 的请求

#### 日志

- entry：每个事件成为 entry，只有 Leader 能创建 entry，entry 的内容为 `<term, index, cmd>`，cmd 是可以应用到状态机的操作
- log：由 entry 构成的数组，每个 entry 都有一个表明自己在 log 中的 index，只有 Leader 才能改变其他节点的 log，entry 总是先被 Leader 添加到自己的 log 数组中，然后再发起共识请求，获得同意后才会被 Leader 提交给状态机，Follower 只能从 Leader 获取新日志和当前的 commitIndex，然后把对应的 entry 应用到自己的状态机中

#### 选举

raft 使用心跳机制触发 Leader 选举

Leader 会周期性地向所有 Follower 发送心跳保证自己的 Leader 地位

Follower 只要能收到来自 Leader/Candidate 的有效信息，就会一直保持 Follower 状态，并刷新 electionElapsed，重新计时，若在计时结束后为收到心跳信息，此时 Follower 认为选举超时，即当前没有可用的 Leader

为了开始进行选举以选出新的 Leader，Follower 会自增自己的 term 号且状态改为 Candidate，然后向所有节点发起 RequestVoteRPC 请求，要求投票给自己，直到选举结束后才转换为 Leader/Follower

在 Candidate 等待投票时，可能会收到当前 Leader 的心跳，此时有两种情况：

- Leader.term >= Candidate.term：说明此次选举已结束，由其他 Candidate 担任 Leader，此时当前服务器会回退为 Follower
- Leader.term < Candidate.term：说明是来自上一个任期的 Leader， 此时会拒绝请求并要求该节点更新 term

需要收到过半的票才能赢得选举，故在同一时间内出现大量 Candidate 时可能导致无法选举出 Leader 而无限重复选举

raft 通过令每个 Candidate 发起选举后随机化一个选举超时时间，使得各个服务器能够分散开来，大多数情况下只会有一个服务器率先超时并被选举为新的 Leader，即每个 Follower 的选举超时计时器不同，不会同时超时

#### 日志复制

一旦选出 Leader，它就开始接收客户端的请求，每个客户端的请求都包含一条需要被复制状态机执行的命令

Leader 收到客户端请求后，会生成一个 entry，在将 entry 添加到自己的 log 末尾后，向所有节点广播该 entry，要求其他节点复制这条 entry

Follower 同意接受该 entry，则会将 entry 添加到自己的日志后面，同时返回给 Leader 同意，Leader 收到多数成功响应后会将 entry 应用到自己的状态机中，即提交 entry，并向客户端返回执行结果

raft 保证以下两个性质：

- 拥有相同 index 和 term 的 entry 一定有相同的 cmd
  - 仅有 Leader 能生成 entry，保证其他节点新增的 entry 一定是同一份
- 拥有相同 index 和 term 的 entry 前面的 entry 也一定相同
  - 需要一致性检查来保证

Leader 的崩溃会导致 Leader 与 Follower 的日志不一样，此时一致性检查失败，Leader 通过强制 Follower 复制自己的日志来处理日志的不一致，即产生冲突时 Follower 上的日志会被覆盖

Leader 通过找到 Follower 与它日志一致的地方，然后删除 Follower 在这之后的日志，再把之后的日志发送给 Follower

Leader 给每个 Follower 维护了一个 nextIndex，表示 Leader 将要发送给该 Follower 的下一条日志目录索引，当一个 Leader 开始掌权时，会将 nextIndex 初始化为它的最新的日志条目索引数 + 1，若 Follower 的日志与 Leader 不一致，AppendEntries 一致性检查会在下一次 AppendEntries RPC 时返回失败，在失败后，Leader 会将 nextIndex 递减后重试，最终 nextIndex 会达到一个 Leader 与 Follower 日志一致的地方，此时 AppendEntries 会返回成功，Follower 中冲突的日志被移除且添加缺少的 Leader 日志条目，一旦 AppendEntries 返回成功，Follower 与 Leader 日志一致，该状态会保持到任期结束

#### 安全性

- 选举限制：
  - Leader 需要保证自己存储全部已提交的日志，从而保证日志只会从 Leader 流向 Follower(Leader 不需要从其他地方获取日志)，Leader 永远不会覆盖已经存在的日志
  - 每个 Candidate 在发起 RequestVoteRPC 时，会带上自身最后一个 entry 的信息，其他节点收到请求后会进行对比，若自身的 entry 更新，则拒绝为该 Candidate 投票(保证选出的 Leader 保存所有已提交日志)
  - 判断 entry 新旧：先比较 term，再比较 index
- 节点崩溃：
  - 从 Leader 崩溃到选举超时的期间整个集群对外不可用
  - Follower/Candidate 崩溃时，发送给它的 RequestVoteRPC 和 AppendEntriesRPC 会失败，raft 的所有请求都是幂等的，所有失败的话就会无限重试，在崩溃恢复后也能收到新的请求，此时再选择追加或拒绝 entry
- 时间与可用性：
  - raft 要求安全性不依赖于时间，即系统不能仅仅因为一些事件发生的比预想时间不同而产生错误，故需要满足以下时间条件：`broadcastTime << electionTimeout << MTBF`
    - broadcastTime：向其他节点并发发送消息的平均响应时间，使 Leader 能够持续发送心跳信息来阻止 Follower 开始选举
    - electionTimeout：选举超时时间，尽量减少 Leader 崩溃到选举超时这段不可用的时间
    - MTBF：单台机器的平均健康时间

## Gossip 协议

分布式系统中不同节点之间需要进行数据共享

集中式发散消息：一个主节点负责广播共享所有信息，其他节点都只与主节点通信，适用于中心化系统，实现简单，但存在单点故障，且同步消息的效率低

Gossip 协议基于==分散式发散消息==，其主要特点为 ==随机传播特性==，是一种随机且带有传染性的方式将信息传播到整个网络，并在一定时间内，使得系统内所有节点状态最终保持一致

==Gossip 协议是一种允许在分布式系统中共享状态的去中心化通信协议，通过这种通信协议，可以将信息传播给网络或集群中的所有成员==

Redis Cluster 就是基于 Gossip 协议实现集群中各个节点数据的最终一致性，Redis Cluster 的节点间会相互发送多种 Gossip 消息(MEET/PING/PONG/FAIL)

### 优势

- 简单：Gossip 原理和实现较为简单
- 动态节点：允许网络中节点的随意增减、宕机、重启，因为 Gossip 协议下的节点是平等的，去中心化的，即新增或重启的节点理论上最终一定会和其他节点状态达到一致
- 速度快：相较于中心传播的模式，Gossip 在节点数量较多的情况下效率更高

### 缺陷

- 达到状态一致的耗时：消息往往需要多个轮次才能完全传播到整个网络，在此过程中部分节点的数据也会发生改变，导致各节点状态不一致，==Gossip 只保证最终一致性，但到达最终一致所需的时长不确定==
- 恶意节点：由于拜占庭将军问题，不允许出现恶意节点
- 消息冗余：消息的传播方向是随机的，即同一个节点可能会收到重复的消息

### Gossip 协议消息传播模式

Gossip 设计了两种可能的消息传播模式：反熵和传谣

#### 反熵

熵可以看作节点之间数据的差异性，反熵即消除不同节点中数据的差异，提升节点间数据的相似度，从而降低熵值

集群中的节点每隔一段时间就随机选择某个其他节点，然后通过互相交换自己的所有数据来消除两者之间的差异，实现数据的最终一致性

实现反熵主要有推、拉、推拉三种方式：

- 推：将自己的所有副本数据推给对方，修复对方副本中的熵
- 拉：拉取对方的所有副本数据，修复自己副本中的熵
- 推拉：同时修复自身与对方副本中的熵

在实际应用场景中，可以设计一个闭环，能够在一个确定的时间范围内实现各个节点数据的最终一致性

![](https://javaguide.cn/assets/%E5%8F%8D%E7%86%B5-%E9%97%AD%E7%8E%AF-BPAGw_p4.png)

反熵实现简单，但不适用于节点过多或节点动态变化的情况，此时需要使用 传谣

#### 传谣

传谣指分布式系统中的一个节点一旦有了新数据后，就会变为活跃节点，活跃节点会周期性地联系其他节点向其发送新数据，直到所有节点都存储了该新数据

![](https://javaguide.cn/assets/gossip-rumor-mongering-D0IpXnM4.gif)

传谣虽然适用于节点数量多的场景，但也需要控制传播的信息包大小，避免网络消耗过大

## 网关

网关类似于 AOP 中的切面，在微服务中，一个系统被拆分为多个服务，但安全认证、流量控制、日志、监控等功能在每个服务中都需要，网关就是用来统一管理这些功能的全局视图

![](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/api-gateway-overview.png)

### 功能

网关主要负责：请求转发 + 请求过滤，需要保证网关服务的高可用，避免单点故障

绝大多数网关能够提供以下功能：

- 请求转发：将请求转发至对应的服务
- 负载均衡：根据各个微服务实例的负载情况或具体的负载均衡策略配置对请求实现动态的负载均衡
- 安全认证：对用户请求进行身份校验
- 参数校验：参数映射与校验
- 日志记录：记录所有请求的行为
- 监控告警：从业务指标、机器指标、JVM 指标等方面进行监控并提供配套的告警机制
- 流量控制：对请求的流量进行控制，限流
- 熔断降级：实时监控请求的统计信息，达到配置的失败阈值后自动熔断，返回默认值
- 响应缓存：当请求静态或更新不频繁的数据时，且短时间内收到多次请求时，可以直接缓存的返回的响应，之后就无需访问业务服务，可以直接返回
- 响应聚合：某些情况下用户可能请求多个业务服务的响应，网关作为业务服务的调用方可以把多个服务的响应聚合起来返回给用户
- 灰度发布：将请求动态分流到不同的服务版本
- 异常处理：对于业务服务返回的异常，网关能够转换为返回给客户的异常，同时进行日志处理
- API 文档：将 API 暴露给组织外的开发人员时需要使用 API 文档(Swagger)
- 协议转换：通过协议转换整合后台基于 REST、AMQP、Dubbo 等不同风格和实现技术的微服务，面向 Web Mobile、开放平台等特定客户端提供统一服务
- 证书管理：将 SSL 整数部署到 API 网关，由一个统一的入口管理接口，降低了证书更换时的难度

![](https://oss.javaguide.cn/github/javaguide/distributed-system/api-gateway/up-35e102c633bbe8e0dea1e075ea3fee5dcfb.png)

## 分布式 Id

Id 是数据的唯一标识，分布式 Id 即分布式系统下的 Id，是分布式系统中不可缺少的一环

一个最基本的分布式 Id 需要满足以下要求：

- 全局唯一：首先要满足的
- 高性能：分布式 Id 的生成速度要快，对本地资源消耗小，因为分布式 Id 的生成需求可能很大
- 高可用 ：生成分布式 Id 的服务要保证可用性无限接近 100%，因为无法生成分布式 Id 可能导致整个系统不可用
- 方便易用

好的分布式 Id 还需要满足：

- 安全：Id 不包含敏感信息
- 有序递增：分布式 Id 存储于数据库时，有序的 Id 可以提升数据库写入速度，且可以直接使用 Id 进行排序
- 具体的业务含义：生成的 Id 与具体业务有关时，可以通过 Id 直接确定出现问题的业务
- 独立部署：分布式系统单独有一个发号器服务，专门用于生成分布式 Id，将 Id 生成与业务服务解耦，但也增加网络调用，更加适用于分布式 Id 需求较大的场景

### 常见解决方案

#### 数据库

##### 主键自增

使用 `repalce into` 代替 `insert into` 插入数据：

- 先尝试将数据插入表中
- 若主键或唯一索引字段冲突而插入失败，先从表中删除含有重复关键字值的冲突行，然后再次尝试把数据插入表中(同更新操作)

优点：

- 实现简单
- Id 有序递增，存储消耗空间小

缺点：

- 支持并发量不大
- 数据库单点问题，可以通过集群解决，但增大了复杂度
- Id 没有具体业务含义
- 安全问题，如单调递增的订单 Id 可以推算出每日的订单量
- 每次获取 Id 都需要访问数据库，效率较低且增大数据库压力

##### 号段模式

针对每次获取 Id 都需要访问数据库的情况进行优化，批量获取数据库中的 Id 存入内存，之后可以直接在内存中读取

虽然减少了数据库访问次数，减轻数据库压力，但仍存在单点问题、Id 没有业务含义、安全问题

#### NoSQL

通过 Redis 的 incr 命令即可实现对 Id 的原子顺序递增，但也存在关系型数据库中的问题

MongoDB 的 ObjectId 也可以用于生成有序递增的分布式 Id 且性能较好，但需要解决重复 Id 的问题(基于机器时间生成)，也存在安全问题(Id 生成有规律)

#### 算法

##### UUID

Universally Unique Identifier 通用唯一标识符，包含 32 个 16 进制数字

不同版本对应的 UUID 生成规则不同：

- 版本 1：UUID 根据时间和节点 Id(通常为 MAC 地址)生成
- 版本 2：UUID 根据标识符(通常是组或用户 Id)、时间和节点 Id 生成
- 版本 3/5：确定性 UUID 通过哈希命名空间标识符和名称生成
- 版本 4：UUID 使用随机性和伪随机性生成

JDK 中通过 UUID 的 randomUUID() 方法生成的 UUID 版本默认为 4

UUID 虽然能保证唯一性且生成速度快，但并不适用于数据库主键：

- UUID 消耗的存储空间较大，而数据库主键要求尽量短
- UUID 是无序的，InnoDB 中无序的主键会严重影响性能
- UUID 基于 MAC 地址生成时可能导致 MAC 地址泄露
- 没有具体的业务含义
- 机器时间不对时可能产生重复 Id

##### Snowflake

雪花算法由 64 bit 的二进制数字组成，这 64 bit 的二进制被分成了几部分，每一部分存储的数据都有特定的含义：

![](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/snowflake-distributed-id-schematic-diagram.png)

- sign：符号位，标识正负，始终为 0，代表生成的 Id 为正数
- timestamp：表示时间戳，单位为毫秒
- datacenter id + worker id：前 5 位表示机房 Id，后 5 位表示机器 Id，但并不绝对
- sequence：表示自增序列号，几单台机器每毫秒能够生成的最大 Id 数

优点：

- 生成速度快
- Id 有序递增
- 灵活，便于改造

缺点：

- Id 生成依赖时间，出现时间回拨情况(服务器上的时间倒退回之前的时间)时可能产生重复 Id
- Id 生成依赖机器 Id，对分布式环境不友好，当需要自动启动/停止或增减机器时，固定的机器 Id 不够灵活

#### 开源框架

- UidGenerator：基于雪花算法，可以自定义组成唯一 Id 的参数
- Leaf：
  - 号段模式：增加双号段避免获取 DB 在获取号段的时候阻塞请求获取 Id 的线程，即一个号段未用完前主动去获取下一个号段
  - Snowflake：解决时间回拨下的问题
- Tinyid：基于号段模式，通过 HTTP 请求向发号器服务申请唯一 Id
  - 双号段缓存
  - 增加多 db 支持：每个 DB 都能生成唯一 Id
  - 增加 tinyid-client：纯本地操作，无需 HTTP 请求

### 分布式 Id 设计指南

#### 订单系统

##### 一码付

一个二维码可以使用支付宝或微信进行扫码支付，二维码本质是一个字符串，聚合码本质是一个链接，用户扫码付钱无需担心支付宝扫了微信的收款码或微信扫了支付宝的收款码，极大地减少了用户的操作

其原理是客户用 APP 扫码后，网站后台会判断客户的扫码环境，即根据打开链接浏览器的 HTTP header，任何浏览器打开 http 链接时，请求的 header 都会有 User-Agent 信息

UA 是特殊字符串头，服务器依次可以识别出客户使用的操作系统及版本、CPU 类型、浏览器信息等

##### 订单号

订单号在实际业务过程中作为一个订单的唯一标识码存在，一般实现以下业务场景：

- 定位出现问题的用户订单
- 对指定订单进行操作，如线下收款，订单核销
- 系统内部对订单流程的处理和跟进

由于订单号生成规则的唯一性，搜索订单相关信息时都是以订单 Id 作为唯一标识符，处理 Id 服务必要的特性外，在订单号的设计上需要体现几个特性：

- 信息安全：订单号不能包含敏感信息(客户个人信息)，也不应该由明显的整体规律(由一个订单号能够轻易推断出其他订单号)
- 部分可读：位数要便于操作，及订单号位数适中且局部有规律，这样便于在订单异常或退款时客服查询
  - 过长或易读性差的订单号会导致输入困难、易错率高，影响售后体验，因此实际业务场景中，订单号的设计通常会携带一些允许公开的对使用场景有帮助的信息(时间、日期、类型)
- 查询效率：常见的订单号由纯数字组成，兼具可读性的同时，int 类型的查询效率更高

##### 优惠券/兑换券

分为两种场景：

- Id 即时生成：电商平台领取的优惠券，只需要在用户领取时分配优惠券信息即可
- 需要预先生成：线上线下结合的场景，如瓶盖、京东卡、超市卡等
  - 预先生成，在活动正式开始前提供出来为活动预热
  - 优惠券体量较大，以万为单位，通常在 10 万级别以上
  - 不可破解、仿制
  - 支持用后核销
  - 优惠券属于广撒网策略，一般利用率低，故==不适合使用数据库存储(占用大量空间，且有效数据少)==

优惠券生成策略应该支持：

- 预先生成
- 校验
- 内容简洁
- 唯一性

为了可读性，一般选择数字 + 英文大小写作为编码空间

预先生成的兑换码不需要额外的存储空间保存信息，每个优惠方案都有独立的一组兑换码，即每次活动的兑换码是独立的，活动 A 的不能用于活动 B 的，每个兑换码有自己的编号防止重复，为了保证兑换码的有效性，对兑换码的数据进行校验，则兑换码的数据组成为：

- 优惠方案 Id：代表当前优惠方案的 Id 号，优惠方案的空间范围决定了可以组织的优惠活动的次数
- 兑换码序列号：代表当前兑换码是当前活动中的第 i 个兑换码，兑换码序列号的空间范围决定了优惠活动可以发行的兑换码数目
- 校验码：校验兑换码是否有效，主要为了快速校验兑换码信息是否正确，其次可以起到填充数据的目的，增强数据的散列性

#### Tracing

##### 日志跟踪

在分布式服务架构下，一个 Web 请求从网关流入，可能会调用多个服务对请求进行处理，拿到最终结果，这个过程中每个服务之间的通信都是单独的网络请求，无论请求经过哪个服务出现故障或处理过慢都会对前端造成影响

处理一个 Web 请求要调用的多个服务，为了更方便的查询哪个环节的服务出现问题，通常会为整个系统引入分布式链路跟踪

在分布式链路跟踪中的两个重要概念：

- 跟踪(trace)：请求在分布式系统中的整个链路视图
- 跨度(span)：代表整个链路中不同服务内部的视图，span 组合在一起就是整个 trace 的视图

在整个请求的调用链中，请求会一直携带 traceid 往下游服务传递，每个服务内部也会生成自己的 spanid 用于生成自己的内部调用视图，并和 traceid 一起传递给下游服务

###### TraceId 生成规则

traceid 除了要求唯一，还要求生成的效率高、吞吐量大，traceid 需要具备接入层的服务器实例自主生成的能力，如果每个 trace 中的 Id 都需要请求公共的 Id 服务生成，则会浪费网络带宽资源，且会阻塞用户请求向下游传递，响应耗时上升，增加没有必要的风险，故==需要服务器实例最好能够自行计算 traceid，spanid，避免依赖外部服务==

生成规则：

- 服务器 IP：由产生 traceid 的机器的 IP 生成
- Id 产生的时间
- 自增序列
- 当前进程号：防止单机多线程出现 traceid 冲突的情况

###### SpanId 生成规则

span 是层的意思，如第一个实例算第一层，请求代理或分流到下一个实例处理，就是第二层，以此类推，通过层，SpanId 代表本次调用在整个调用链路树中的位置

假设一个服务器实例 A 接收了一次用户请求，代表整个调用的根节点，那么 A 层处理这次请求产生的非服务调用日志记录 spanid 的值都是 0，A 层需要通过 RPC 一次调用 B、C、D 三个服务器实例，那么在 A 的日志中，spanid 依次为 0.1、0.2、0.3，在 B、C、D 中也是 0.1、0.2、0.3，如果 C 系统在处理请求时又调用了 E、F 两个服务器实例，那么 C 系统中对应的 spanid 时 0.2.1、0.2.2，E、F 对应的日志也为 0.2.1、0.2.2

如果把一次调用中的所有 spanid 组合起来，可以组成一颗完整的链路树

==spanid 的生成本质：在跨层传递请求的同时，控制大小版本号的自增来实现==

#### 短网址

短网址主要功能包括网址缩短与还原两大功能，相对于长网址，短网址可以更方便第在电子邮件、社交网络、手机上传播，如原本很长的网址通过短网址服务即可生成相应的短网址，避免拆行或超出字符限制

![](https://oss.javaguide.cn/github/javaguide/system-design/distributed-system/distributed-id-design-short-url.png)

常用的 Id 生成服务比如：MySQL Id 自增，Redis 键自增、号段模式、生成的 Id 都是一串数字，短网址服务把客户的长网址转换成短网址

## 分布式锁

多线程环境中，多个线程同时访问共享资源(商品库存、外卖订单)会发生数据竞争，可能会出现脏数据或系统问题，威胁到程序正常运行(超卖问题)

为了保证共享资源被安全访问，需要使用互斥操作对共享资源进行保护，即同一时刻只允许一个线程访问共享资源，其他线程需要等待当前线程释放后才能访问，这样可以避免数据竞争和脏数据问题，保证程序的正确性和稳定性，悲观锁是比较通用的解决方案

悲观锁总是假设最坏的情况，认为共享资源每次被访问的时候就会出现问题(数据被修改)，所以每次获取资源都会上锁，其他线程想拿到资源就会被阻塞直到锁被上一个持有者释放，即共享资源保证每次只给一个线程使用，其他线程阻塞，用完后再把资源转让给其他线程

对于单机多线程，在 Java 中常用 ReentrantLock 类、synchronized 关键字等 JDK 自带的 ==本地锁== 来控制一个 JVM 进程内的多个线程对本地共享资源的访问

![](https://oss.javaguide.cn/github/javaguide/distributed-system/distributed-lock/jvm-local-lock.png)

线程访问共享资源是互斥的，同一时刻只有一个线程可以获取到本地锁访问共享资源

==分布式系统下，不同的服务运行在独立的 JVM 进程上==，如果多个 JVM 进程共享同一份资源，使用本地锁就无法实现资源的互斥访问，故需要==分布式锁令不在同一进程内的多个线程能获取同一把锁==

![](https://oss.javaguide.cn/github/javaguide/distributed-system/distributed-lock/distributed-lock.png)

### 分布式锁特性

基本的分布式锁需要：

- 互斥：任意时刻，锁只能被一个线程持有
- 高可用：==锁服务是高可用的==，当一个锁服务出现问题，能够自动切换到另外一个锁服务，并且即使客户端释放锁的逻辑出现问题，==锁最终一定会被释放(超时机制)==，不会影响其他线程对共享资源的访问
- 可重入：已经获取锁的节点可以再次获取锁

好的分布式锁还需要：

- 高性能：锁的获取与释放应该快速完成，减小对整个系统性能的影响
- 非阻塞：获取不到锁时不能无限等待

### 实现

- 基于关系型数据库：MySQL，通过唯一索引或排他锁实现，性能较差且不具备锁失效机制
- 基于分布式协调服务：ZooKeeper
- 基于分布式键值存储系统：Redis

#### 基于 Redis 实现

Redis 的 SETNX 可以实现互斥，当 key 不存在时才会设置 key 的值，key 已存在时什么都不做，需要释放锁时可以直接 DEL 删除对应的 key

为了防止误删到其他的锁，可以使用 Lua 脚本通过 key 对应的 value(唯一值)进行判断，Lua 脚本可以保证解锁操作的原子性，即 Redis 在执行 Lua 脚本时可以以原子性的方式执行

![](https://oss.javaguide.cn/github/javaguide/distributed-system/distributed-lock/distributed-lock-setnx.png)

##### 过期时间

为了保证锁最终一定会被释放，需要为锁设置过期时间：

```bash
SET lockKey uniqueValue EX 3 NX
```

- lockKey：锁名
- uniqueValue：标识锁的字符串
- NX：if Not eXists，表示互斥，只有 lockKey 对应的 key 值不存在时才进行操作
- EX：过期时间，单位为秒

指定 key 与过期时间的操作应该是原子操作，否则仍会出现锁无法被释放的问题(设置过程中被其他线程修改导致过期时间不断被更新)

##### 自动续期

若锁过期时，操作还未完成，则会导致分布式锁失效(当前线程未持有锁却能获取资源)，太长的过期时间则会影响性能，故需要锁能在过期时自动续期

Redisson 中分布式锁自带自动续期，其提供了一个专门用来监控和续期锁的 Watch Dog，若操作共享资源的线程还未执行完成，Watch Dog 会不断延长过期时间，保证锁不会因为超时而被释放

![](https://oss.javaguide.cn/github/javaguide/distributed-system/distributed-lock/distributed-lock-redisson-renew-expiration.png)

##### 可重入锁

一个线程在执行一个带锁的方法，该方法中又调用了另一个需要相同锁的方法，则该线程可以直接执行调用的方法即可重入，而无需重新获取锁

可重入锁的实现核心为判断当前获取的锁是否为自身的锁，如果是则不需要重新获取，为此可以为每个锁关联一个可重入计数器和一个占有它的线程，当可重入计数器大于 0 时，锁被占有，需要判断占有该锁的线程和请求获取锁的线程是否为同一个

##### 集群情况下的可靠性

为了避免单点故障，生产环境下的 Redis 服务通常是集群化部署

Redis 集群下，数据同步到各个节点是异步的，如果 Redis 主节点获取到锁后，在没有同步到其他节点前，Redis 主节点宕机，此时新的 Redis 主节点仍然可以获取到锁，导致多个应用服务可以同时获取到锁：

1. 客户端 A 在 Master 获取到锁
2. Master 将数据(包括锁信息)同步给 Slave 过程中宕机，锁没同步成功，Reids 触发 Master 选举
3. Slave 成为新的 Master，但新 Master 上并没有原本锁的 key，即新 Master 并不知道客户端 A 已经获取到锁
4. 客户端 B 向新 Master 获取锁，由于新 Master 上没有锁信息，故可以获取成功
5. 此时客户端 A 与 B 都持有相关的 key 的锁，违反了分布式锁的互斥性

![](https://oss.javaguide.cn/github/javaguide/distributed-system/distributed-lock/redis-master-slave-distributed-lock.png)

Redis 设计了 RedLock 算法解决：==让客户端向 Redis 集群中的多个独立 Redis 实例依次请求申请加锁==，如果客户端能够和半数以上的实例成功完成加锁操作，则可以成功获得分布式锁，否则失败

即使部分 Redis 节点出现问题，只要保证半数以上的节点可用，分布式锁服务就是正常的

RedLock 直接操作 Redis 节点而非通过 Redis 集群操作，由此避免 Redis 集群主从切换导致的锁丢失，故 RedLock 实现复杂，性能较差，在不需要实现绝对可靠的分布式锁的情况下不会使用

## 分布式事务

微服务架构下，一个系统被拆分成多个小的微服务，每个微服务都可能存在不同的机器上，且每个微服务可能都有一个单独的数据库供自己使用，这种情况下，一组操作可能会涉及到多个微服务以及多个数据库

这种跨数据库的场景都需要用到分布式业务，即==分布式事务的目的就是保证系统中多个相关联的数据库中的数据一致性==

### 柔性事务

为了追求高可用，在 CAP 与 BASE 理论的基础上推出柔性事务的概念，==柔性事务追求的是最终一致性==，根据自身业务的特性，通过适当的方式保证系统数据的最终一致性

一般有 TCC、MQ 事务、Saga

### 刚性事务

刚性事务追求的是强一致性，一般有 2PC、3PC，两种解决方案属于业务代码无侵入方案

### 解决方案

#### 2PC

2PC 涉及的一些角色：

- AP：Application Program，程序本身
- RM：Resource Manager 资源管理器，即事务的参与者，绝大部分情况下就是指数据库，一个分布式事务往往涉及多个 RM
- TM：Transaction Manager 事务管理器，负责管理全局事务，分配事务唯一标识，监控事务的执行进度，负责事务的提交、回滚、失败恢复等

2PC 即 Two-Phase Commit 两阶段提交协议：

- 2：事务提交的两个阶段
- P：Prepare 准备阶段，测试 RM 能否执行 ==本地数据库事务==，由此决定事务的提交或回滚 
- C：Commit 提交阶段

优点：

- 实现简单，主流数据库都有自己的实现
- 针对数据强一致性，但仍可能存在数据不一致的情况

缺点：

- 同步阻塞：RM 在正式提交事务前会一直占用相关资源，等待 TM 的 Commit/Rollback 消息
- 数据不一致：网络问题或 TM 宕机都可能造成数据不一致，如 Commit 阶段中，部分网络出现问题导致部分参与者收不到消息，就会导致数据不一致
- 单点问题：TM 为单点，崩溃时整个事务流程无法继续进行

##### 准备阶段

准备阶段的核心是“询问”事务参与者执行本地数据库事务是否成功：

1. TM 向所有涉及的 RM 发送消息询问是否可以执行业务操作
2. RM 收到消息后开始执行本地数据库事务预操作(写 redo log/undo log)，此时==不会提交事务==
3. RM 执行本地数据库事务操作成功，则回复已就绪，否则未就绪

##### 提交阶段

提交阶段的核心是“询问”事务参与者提交本地事务是否成功：

1. 所有 RM 回复就绪状态
2. TM 向所有 RM 发送可以提交事务的消息(Commit 消息)
3. RM 收到 Commit 消息后执行 ==提交本地数据库事务== 操作，执行完成后 ==释放整个事务期间所占用的资源==
4. RM 回复事务已提交(ACK 消息)
5. TM 收到所有 RM 的 ACK 消息后整个分布式事务过程结束
   1. 当任一 RM 未就绪，TM 向所有 RM 发送回滚消息(Rollback 消息)
   2. RM 接收到 Rollback 消息后执行 ==本地数据库事务回滚==，执行完成后 ==释放整个事务期间所占用的资源==
   3. RM 回复事务已回滚
   4. TM 收到所有事务 ACK 消息后中断事务

#### 3PC

3PC 在 2PC 的基础上进行优化，将准备阶段细分为 2 个阶段：

- CanCommit：准备阶段，RM 不执行事务操作，TM 只发送 ==准备请求==，并询问 RM 能否执行本地数据库事务操作
- PreCommit：预提交阶段
  - 若 CanCommit 阶段中所有 RM 都回复 Yes，TM 会向所有 RM 发送 PreCommit 消息，RM 收到后会执行本地数据库事务预操作(redo log/undo log)并返回 Yes/No 的执行结果
  - 若有 RM 回复 NO 或超时，TM 会向所有 RM 发送 Abort 消息，RM 收到后立即中断事务，RM 在 CanCommit 阶段并未执行什么操作，故中断的损失较低
- DoCommit：事务提交阶段，开始进行真正的事务提交
  - PreCommit 阶段所有 RM 回复 Yes，TM 向所有 RM 发送 DoCommit 消息，RM 收到后执行本地数据库事务提交，释放相关资源后返回 Yes/No
  - PreCommit 阶段存在 RM 回复 No 或超时，TM 向所有 RM 发送 Abort 消息，RM 收到后执行本地数据库回滚，释放相关资源后返回 Yes/No
  - ==超时机制==：如果 RM 在设定时间内未收到 TM 的 DoCommit 消息，RM 会忽略 TM 状态，直接进行事务提交以避免长时间阻塞

##### 超时机制

3PC 同时在 TM 和 RM 中引入超时机制，如果在一定时间内没有收到 RM 的消息就默认失败，进而避免 TM 一直阻塞占用资源，2PC 中只有 TM 才有超时机制，当 TM 长时间无法与 RM 通信时，就会导致无法释放资源阻塞的问题

但 3PC 并没有完全解决 2PC 的阻塞和数据一致性问题，且引入了性能等新问题，故 3PC 并没有广泛应用，更多情况下通过复制状态机解决 2PC 的阻塞问题

#### TCC 补偿事务

TCC 分为 3 个阶段：

- Try：尝试阶段，尝试执行，完成业务检查，并预留好必需的业务资源
  - 例如：检查账户余额，预留转账资金
- Confirm：确认阶段，确认执行，当所有 RM 的 Try 阶段执行成功就会执行 Confirm，Confirm 阶段会处理 Try 阶段预留的业务资源，否则执行 Cancel
  - 例如：执行实际的金额变更
- Cancel：取消阶段，取消执行，释放 Try 阶段预留的业务资源
  - 例如：释放预留的转账资金

每个阶段有业务代码控制，可以避免长事务，提高性能，但也增加了业务代码的侵入性，即 ==TCC 不需要依赖底层数据资源的事务支持，但需要手动实现更多代码==

##### Confirm/Cancel 阶段的容错

TCC 会记录事务日志并持久化事务日志到指定的存储介质(本地文件、数据库、ZooKeeper)，事务日志包含事务的执行状态，通过事务执行状态可以判断事务提交成功与否，以及具体失败在哪一步

若发现在 Confirm/Cancel 阶段失败，则会进行重试，继续尝试执行 Confirm/Cancel 阶段的逻辑，如果超过重试次数后还未成功执行，就需要人工处理

当事务成功提交后，事务日志就可以被删除以节省资源

##### 对比 2PC/3PC

- 实现：2PC/3PC 依赖数据库或存储资源层面的事务；TCC 依赖修改业务代码实现
- 侵入性：2PC/3PC 属于业务代码无侵入；TCC 对业务代码有侵入
- 一致性：2PC/3PC 追求强一致性，在两阶段提交的整个过程中一直持有数据库的锁；TCC 追求最终一致性，不会一直持有各个业务资源的锁

#### MQ 事务

常用的消息队列都提供了事务相关的功能，事务允许事件流应用将消费、处理、生产消息整个过程定义为一个原子操作，以 RocketMQ 为例：

![](https://rocketmq.apache.org/assets/images/%E4%BA%8B%E5%8A%A1%E6%B6%88%E6%81%AF2-2673a99678f13a471b8fc0bd4ab3bf3a.png)

- 生产者在消息队列上开启一个事务，发送半事务消息给 MQ Server/Broker，事务提交前，半事务消息对于消费者不可见
- 半事务消息发送成功后，生产者就开始执行本地事务
- 生产者本地事务执行成功后，半事务消息变成正常消息，可以正常被消费
- 生成者本地事务执行失败时，会直接回滚

即 MQ 事务也为两阶段提交，先发送半事务消息，等本地事务执行成功后才变为正常消息

##### 生产者 Commit/Rollback 消息发送失败

RocketMQ 中的 Broker 会定期去生产者上反查事务的本地事务执行情况，并根据反查结果决定提交或回滚事务

事务反查机制依赖于业务代码实现对应的接口，故存在业务代码侵入性

##### 正常消息未被正常消费

消息消费失败时，RocketMQ 会自动进行消费重试，超过最大重试次数时，RocketMQ 会认为消息有问题，并将其放入 ==死信队列==，进入死信队列的消费一般需要人工处理

##### QMQ 实现

QMQ 借助数据库自带的事务功能，降低了实现的复杂度，其核心思想 是 ==本地消息表== 方案，将分布式事务拆分成本地事务进行处理

通过维护一个本地消息表存放消息发送的状态，保存消息发送情况到本地消息表的操作和业务操作要在一个事务中提交，即业务执行成功代表消息表写入成功

之后再单独起一个线程定时轮询消息表，把没处理的消息发送到消息中间件，消息发送成功后，更新消息状态为成功或直接删除消息

不同于 ==RocketMQ 中消息队列挂掉会导致数据库事务无法执行，整个应用也挂掉==；QMQ 中即使消息队列挂了也不影响数据库事务的执行

#### Saga

Saga 属于长事务解决方案，核心思想是将长事务拆分成多个本地短事务：

- 长事务拆分为 T1、T2 ~ Tn 个本地短事务
- 每个短事务都有一个补偿动作 C1、C2 ~ Cn

若所有短事务都能顺利完成，整个事务也顺利结束，否则将采取恢复模式

Saga 没有尝试阶段，所有本地事务都会直接提交，故性能较高，但也因为不会预留资源，不能保证隔离性

##### 反向恢复

如果 Ti 短事务提交失败，则补偿所有已完成的事务(一直执行 Ci 对 Ti 进行补偿)

##### 正向恢复

如果 Ti 短事务提交失败，则一直对 Ti 进行重试，直到成功为止

反向恢复与正向恢复都需要开发者自行实现，故存在业务代码侵入性

##### 容错性

为了应对网络故障、服务器宕机等问题导致补偿操作失败，需要将事务执行操作通过日志记录下来，在系统恢复后可以得知短事务或补偿操作的执行情况

## 分布式配置管理

微服务下，业务的发展会导致服务数量增加，进而导致程序配置增多(服务地址、数据库参数等)，传统的配置文件无法满足需求：

- 安全性：配置放在代码库容易泄露
- 时效性：修改配置需要重启服务器才能生效
- 权限控制：没有对配置的修改、发布等操作进行严格的权限控制
- 配置集中管理：配置文件过于分散、不方便管理
- 版本控制：配置修改的记录，版本管理

一个完备的配置中心需要：

- 权限控制：配置的修改、发布操作需要严格的权限控制
- 日志记录：配置的修改、发布操作需要记录完整的日志，便于后期排查问题
- 配置推送：
  - 推：实时性变更，配置更新后推给应用，需要应用和配置中心保持长连接，复杂度高
  - 送：实时性较差，应用隔一段时间手动拉取配置
  - 推拉结合
- 灰度发布：配置只推给部分应用
- 易操作：提供 Web 界面方便配置修改和发布
- 版本跟踪：所有的配置发布都有版本概念，方便回滚
- 配置回滚
- 可持久化

### 常见的配置中心

- Spring Cloud Config：基于 Git 存储配置，属于 Spring Cloud 生态组件，可以与 Spring Cloud 体系无缝整合
- Nacos：使用简单，可以直接用来做服务发现及管理
- Apollo：只能用于配置管理，使用相对复杂
- K8s ConfigMap：适用于 Kubernetes

## 服务的注册与发现

微服务架构下，一个系统通常由多个微服务组成，一个用户请求可能会需要多个服务参与，这些服务之间互相配合以维持系统的正常运行

在没有服务注册与发现机制前，每个服务会将其依赖的其他服务的地址信息写死在配置文件里，当系统中的某个服务访问量突然变大，需要对该服务进行扩容(增加部署的服务器)，此时就需要手动更新所有其他服务器中的配置，当某个服务器宕机时也是同理，整个过程繁琐且易错

服务注册与发现即是解决这一问题的方案，由注册中心负责维护可用服务列表，通过注册中心动态获取可用服务的地址信息，当服务信息发生变更时，==注册中心会将变更推送给相关联的服务==，自动更新服务地址信息，且无需重启服务，实现服务更加优雅的上下线与弹性扩容

除此之外，服务注册与发现还支持 ==不可用服务剔除==，即注册中心会通过 ==心跳机制== 检测服务是否可用，若服务不可用，则注册中心会主动剔除该服务并将变更推送给相关联的服务

总的来说，注册中心的主要功能有：

- 服务注册与查询
- 服务状态变更通知，不可用服务剔除
- 服务权重配置

### 基本流程

- 服务注册：每个服务节点在启动时，会向注册中心注册服务，即将自己的地址信息(ip、端口、服务名等信息)上报给注册中心，注册中心负责将地址信息保存起来
- 服务发现：一个服务节点要调用另外一个服务节点时，会直接拿着服务的信息找注册中心要对方的地址信息，服务节点拿到后会在本地缓存，保证注册中心宕机后仍能正常调用服务
- 服务信息变更：当服务信息发生变更时，注册中心会将变更推送给相关联的服务，更新服务地址信息
  - 可用性检测：为了保证服务地址列表中都是可用服务的地址信息，注册中心通过心跳机制来检测服务是否可用，不可用的服务会被剔除

### 常见的注册中心

- ZooKeeper：保证 CP，任何时刻对 ZooKeeper 的读请求结果都是一直的，但 Leader 选举过程与半数机器不可用时会导致整体不可用
- Eureka：保证 AP，只需要一台机器就可以正常工作，但不保证数据是最新的
- Nacos：支持 AP 与 CP

## 分布式日志管理

没有日志系统时，每次需要查看日志都需要登录每台机器，然后使用 grep、wc 等 Linux 命令来对日志进行搜索，这个过程非常麻烦且耗时的

==日志系统就是为了对日志进行集中管理==

基本的日志系统需要：

- 采集日志：支持多种日志以及数据源的采集
- 日志数据清洗/处理：采集到的原始日志数据需要先进行清理
- 存储：为了方便对清洗后的日志进行处理，可以对接多种存储方式(ElasticSearch → 日志检索；Hadoop → 离线数据分析)
- 展示与搜索：支持可视化地展示日志，且能够根据关键字快速地定位到日志并查看日志上下文
- 告警：支持对接常见的监控系统

### ELK

ELK 是目前较为常用的开源日志系统解决方案

ELK 日志架构：

![](https://pic4.zhimg.com/80/v2-7261d6564808a0d307c682051c2c3f9f_720w.webp)

### Loki

ELK 日志系统功能丰富，稳定可靠，但对资源消耗较大，且很多功能可能用不上

Loki 即是一个更加轻量级的日志系统